{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing logistic regression\n",
    "\n",
    "A logistic regression \"squashes\" a linear expression into a 0-1 range using a logistic, or sigmoidal, function.  That looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "lim = 6\n",
    "t = np.linspace(-lim, lim, 100)\n",
    "sig = 1 / (1 + np.exp(-t))\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot([-lim, lim], [0, 0], \"k-\")\n",
    "plt.plot([-lim, lim], [0.5, 0.5], \"k:\")\n",
    "plt.plot([-lim, lim], [1, 1], \"k:\")\n",
    "plt.plot([0, 0], [-1.1, 1.1], \"k-\")\n",
    "plt.plot(t, sig, \"b-\", linewidth=2, label=r\"$\\sigma(t) = \\dfrac{1}{1 + e^{-t}}$\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.axis([-lim, lim, -0.1, 1.1])\n",
    "plt.gca().set_yticks([0, 0.25, 0.5, 0.75, 1])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll demonstrate a logistic regression first by using the famous 'iris' dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "list(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only care about data and target right now.  First, let's read the dataset description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, by conventions the \"targets\" correspond t indices in the target names array.  So, the above are all 'setosa'.  As the slmplest form of a logistic reggression is binary classifier, let's focus just on predicting \"virginica\". We'll do that by relabeling our \"targets\".  By convention, we often use the variable `X` to refer to data, and `y` to refer to targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = iris.data.values\n",
    "y = iris.target_names[iris.target] == 'virginica'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a train / test split, and then train the classifier.  See comments in code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split creates four groups.  Random state is set here for replicability purposes, and is optional\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Create the machine learning model - this is typically how we do things in SciKit learn\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "\n",
    "# Train the clasifier\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, great!  Now we can examine performance.  Typically, we'd do this by predicting classes, and the comparing the outcome.  Like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the predictions\n",
    "y_pred = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use any number of scoring routines from scikit learn to test our results. But let's do it manually first (this is just accuracy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_correct = sum(y_pred == y_test)\n",
    "print(n_correct / len(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:\n",
    "\n",
    "Try adding some noise to your data and see how your accuracy changes.  Note that you can add noise to a matrix in numpy like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.ones((10,10))\n",
    "print(\"Original matrix\")\n",
    "print(sample)\n",
    "\n",
    "# 0 is the mean, .3 is the standard deviation\n",
    "# The last parameter is the shape of the matrix we want to retrieve\n",
    "noise = np.random.normal(0,.3,sample.shape)\n",
    "noisy_sample = sample + noise\n",
    "print(\"\\n\\nNoisy matrix\")\n",
    "print(noisy_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're feeling ambitious, create a loop and graph your accuracy across noise levels!  You'll want a function for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(width=120)\n",
    "pp.pprint(mnist.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.keys()  # extra code – we only use data and target in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, data and target exist for most sklearn datasets.  Data is a matrix of features, and target is a vector of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist.data, mnist.target\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanted to see these as a DataFrame, you could do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(mnist.data)\n",
    "df[\"target\"] = mnist.target\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that's not really necessary here.  We'll just use these things as matrices and arrays.  Let's look at the shape here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X shape = {X.shape}\")\n",
    "print(f\"y shape = {y.shape}\")\n",
    "print(f\"Is number of rows equal? {X.shape[0] == y.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where does the 784 come from?  Read the docs above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "28 * 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one of these images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define function to plot a single digit\n",
    "def plot_digit(image_data):\n",
    "    image = image_data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "some_digit = X[0]\n",
    "plot_digit(some_digit)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 9))\n",
    "for idx, image_data in enumerate(X[:100]):\n",
    "    plt.subplot(10, 10, idx + 1)\n",
    "    plot_digit(image_data)\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the classfier\n",
    "\n",
    "Once again, since we're using logistic regression, let's just go with predicting '5s'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Setting up the data here\n",
    "# Though it's not really necessary here (MINST variables are all roughly the same) it's often useful to z-score\n",
    "# your data.  Especially important for logistic regression\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Arbitratily taking the first 60000 rows here\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "\n",
    "y_train_5 = (y_train == '5')  # True for all 5s, False for all other digits\n",
    "y_test_5 = (y_test == '5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm going to use the SGDClassifier here - it's a lot like a logistic regression with an \"sag\" solver,\n",
    "# but is a lot faster with larger data sets\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_train, y_train_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall we already checked that X[0] is a five\n",
    "some_digit = X[0]\n",
    "\n",
    "# Note that predict expect an array of values, so we tuck this into an array before testing.\n",
    "sgd_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring accuracy with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Scikit learn makes it easy to use cross validation with simple measures\n",
    "# CV is the number of folds\n",
    "cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "Go ahead and dig into the docs to use the following scoring methods, reporting the average for each (note `np.mean` will return average over an array)\n",
    "\n",
    "1. Precision\n",
    "2. Recall\n",
    "3. F1 scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want more control over your classifier, use the `KFold` and related classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits=3)  # add shuffle=True if the dataset is not\n",
    "                                       # already shuffled\n",
    "for train_index, test_index in skfolds.split(X_train, y_train_5):\n",
    "    # It's a good idea to use a fresh, untrained model each time you run on new data\n",
    "    # The \"clone\" command does that, but simplifies things by copying other parameters\n",
    "    clone_clf = clone(sgd_clf)\n",
    "    X_train_folds = X_train[train_index]\n",
    "    y_train_folds = y_train_5[train_index]\n",
    "    X_test_fold = X_train[test_index]\n",
    "    y_test_fold = y_train_5[test_index]\n",
    "\n",
    "    clone_clf.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_clf.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct / len(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems quite good!  But also always useful to compare to a naive classifer.  The `DummyClassifier` is one such instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dummy_clf = DummyClassifier()\n",
    "cross_val_score(dummy_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "Instead of `cross_val_score` we can just use `cross_val_predict` to get the raw predictions - sklearn takes care of compiling our results so they are easy to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n",
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_5.shape == y_train_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You've hopefully already found the sklearn metrics library\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_train_5, y_train_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to get a sense of things, see what happens if we had a perfect predictor\n",
    "\n",
    "y_train_perfect_predictions = y_train_5  # pretend we reached perfection\n",
    "confusion_matrix(y_train_5, y_train_perfect_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision_score(y_train_5, y_train_pred)  # == 3530 / (687 + 3530)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same computation, using the confusion matrix above\n",
    "# TP / (FP + TP)\n",
    "cm[1, 1] / (cm[0, 1] + cm[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_train_5, y_train_pred)  # == 3530 / (1891 + 3530)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TP / (FN + TP)\n",
    "cm[1, 1] / (cm[1, 0] + cm[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating by hand\n",
    "cm[1, 1] / (cm[1, 1] + (cm[1, 0] + cm[0, 1]) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision/ Recall Trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"decision_function\" method returns the raw value, of the predictor, which is then \n",
    "# thresholded to achieve an outcome\n",
    "\n",
    "y_scores = sgd_clf.decision_function([some_digit])\n",
    "y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can plug this directly into cross_val_predict to get the scores across all of our data\n",
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n",
    "                             method=\"decision_function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit learn gives us a really nice way to look at this!\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "threshold = 3000\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n",
    "plt.figure(figsize=(8, 4))  # extra code – it's not needed, just formatting\n",
    "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "plt.vlines(threshold, 0, 1.0, \"k\", \"dotted\", label=\"threshold\")\n",
    "\n",
    "idx = (thresholds >= threshold).argmax()  # first index ≥ threshold\n",
    "plt.plot(thresholds[idx], precisions[idx], \"bo\")\n",
    "plt.plot(thresholds[idx], recalls[idx], \"go\")\n",
    "plt.axis([-50000, 50000, 0, 1])\n",
    "plt.grid()\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.legend(loc=\"center right\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can graph these two together:\n",
    "\n",
    "import matplotlib.patches as patches  # extra code – for the curved arrow\n",
    "\n",
    "plt.figure(figsize=(6, 5))  # extra code – not needed, just formatting\n",
    "\n",
    "plt.plot(recalls, precisions, linewidth=2, label=\"Precision/Recall curve\")\n",
    "\n",
    "plt.plot([recalls[idx], recalls[idx]], [0., precisions[idx]], \"k:\")\n",
    "plt.plot([0.0, recalls[idx]], [precisions[idx], precisions[idx]], \"k:\")\n",
    "plt.plot([recalls[idx]], [precisions[idx]], \"ko\",\n",
    "         label=\"Point at threshold 3,000\")\n",
    "plt.gca().add_patch(patches.FancyArrowPatch(\n",
    "    (0.79, 0.60), (0.61, 0.78),\n",
    "    connectionstyle=\"arc3,rad=.2\",\n",
    "    arrowstyle=\"Simple, tail_width=1.5, head_width=8, head_length=10\",\n",
    "    color=\"#444444\"))\n",
    "plt.text(0.56, 0.62, \"Higher\\nthreshold\", color=\"#333333\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.grid()\n",
    "plt.legend(loc=\"lower left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this analysis, we can arbitrarily obtain a threshold to achieve a given level of precision or recall:\n",
    "\n",
    "idx_for_90_precision = (precisions >= 0.90).argmax()\n",
    "threshold_for_90_precision = thresholds[idx_for_90_precision]\n",
    "threshold_for_90_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "1. What is the precision score at a threshold of 90?\n",
    "2. What is recall at this threshold?  F1 score?\n",
    "3. Can you do the same thing, except optimizing to achieve recall > .6?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SciKit learn also gives us ROC curves for free\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n",
    "\n",
    "idx_for_threshold_at_90 = (thresholds <= threshold_for_90_precision).argmax()\n",
    "tpr_90, fpr_90 = tpr[idx_for_threshold_at_90], fpr[idx_for_threshold_at_90]\n",
    "\n",
    "\n",
    "plt.plot(fpr, tpr, linewidth=2, label=\"ROC curve\")\n",
    "plt.plot([0, 1], [0, 1], 'k:', label=\"Random classifier's ROC curve\")\n",
    "plt.plot([fpr_90], [tpr_90], \"ko\", label=\"Threshold for 90% precision\")\n",
    "\n",
    "# just beautifies the figure\n",
    "plt.gca().add_patch(patches.FancyArrowPatch(\n",
    "    (0.20, 0.89), (0.07, 0.70),\n",
    "    connectionstyle=\"arc3,rad=.4\",\n",
    "    arrowstyle=\"Simple, tail_width=1.5, head_width=8, head_length=10\",\n",
    "    color=\"#444444\"))\n",
    "plt.text(0.12, 0.71, \"Higher\\nthreshold\", color=\"#333333\")\n",
    "plt.xlabel('False Positive Rate (Fall-Out)')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.grid()\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.legend(loc=\"lower right\", fontsize=13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing with Scikit Learn\n",
    "\n",
    "SciKit Learn has a rich API for splitting your data into train and test sets.  I'll review some of these methods here.\n",
    "\n",
    "As a working example, we'll use the `wine` dataset, which is focused on predicting the quality of wine based on it's chemical makeup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "wine_data = pd.read_csv(url, delimiter=\";\")\n",
    "wine_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll divide wines into `good` quality and `bad` quality by selecting all wines with a quality score of 7 or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary classification target variable\n",
    "wine_data['quality_label'] = wine_data['quality'].apply(lambda x: 1 if x >= 7 else 0)\n",
    "\n",
    "# Features and Target\n",
    "X = wine_data.drop(['quality', 'quality_label'], axis=1)\n",
    "y = wine_data['quality_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Train-Test Split\n",
    "\n",
    "The simplest way to evaluate the performance of a machine learning algorithm is to use different datasets for training and testing. Scikit-learn has a `train_test_split` function that makes it easy to divide your data.  We did this above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Let's scale our data first\n",
    "\n",
    "\n",
    "# train test split creates four groups.  Random state is set here for replicability purposes, and is optional\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "# Create the machine learning model - this is typically how we do things in SciKit learn\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "\n",
    "# Train the clasifier\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "# Here are the predictions\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "n_correct = sum(y_pred == y_test)\n",
    "print(n_correct / len(y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "\n",
    "In cross-validation, the data is divided into `k` subsets (or \"folds\"). The model is trained on `k-1` of these subsets and tested on the remaining one. This process is repeated `k` times, each time with a different subset as the test set.\n",
    "\n",
    "#### K-Folds\n",
    "\n",
    "The standard `KFold` cross-validator divides the dataset into `k` different folds. During each round of training and testing, a different fold is used as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "# Initialize Logistic Regression and StandardScaler\n",
    "clf = LogisticRegression(random_state=42)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Initialize list to store accuracy for each fold\n",
    "accuracy_list = []\n",
    "\n",
    "# Loop through each fold\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split the data into current train and test set\n",
    "    \n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "\n",
    "\n",
    "    # Scale the features\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Fit the model\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accuracy_list.append(acc)\n",
    "\n",
    "# Calculate and print the average accuracy\n",
    "average_accuracy = sum(accuracy_list) / len(accuracy_list)\n",
    "print(f'Average Accuracy: {average_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKLearn's `cross_val_score` validator simplifies this process.  In order to scale each data set separately though, we'll have to use a `Pipeline`, which chains together multiple operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline with StandardScaler and LogisticRegression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Step 1: Scale the data\n",
    "    ('classifier', LogisticRegression())  # Step 2: Train a logistic regression model\n",
    "])\n",
    "# Use cross_val_score to get the scores for each fold\n",
    "scores = cross_val_score(pipeline, X, y, cv=5)\n",
    "\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean cross-validation score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn provides a variety of scoring methods that can be passed as a parameter to functions like `cross_val_score`. The `scoring` parameter allows you to specify a string representing a built-in scoring metric or even pass a custom scoring function.\n",
    "\n",
    "Here's how you can do it with F1 score.  Note the 'macro' designator indicates that we are taking the balanced average of scores across all classes (rather than an average weighted by class membership).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "f1_scores = cross_val_score(pipeline, X, y, cv=5, scoring='f1_macro')\n",
    "print(\"F1 scores for each fold:\", f1_scores)\n",
    "print(\"Mean F1 score:\", f1_scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also build a composite scorer that calculates multiple scores.  In the following, we use `cross_validate` instead of `cross_validate_score`, because the later will only return an array of single scores.  We'll also include `train_scores` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scoring = {\n",
    "    'f1': 'f1_macro',\n",
    "    'precision': make_scorer(precision_score, average='macro'),\n",
    "    'recall': make_scorer(recall_score, average='macro')\n",
    "}\n",
    "\n",
    "# Using multiple metrics\n",
    "scores = cross_validate(pipeline, X, y, cv=5, scoring=scoring,return_train_score=True)\n",
    "for key, values in scores.items():\n",
    "    print(f\"{key}: {values.mean():.3f} (+/- {values.std() * 2:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note also that while SciKit provides a `StratifiedKFold` class, it does not have a stratified version of the `cross_val_score` function.  However, we can pass a stratified kfold class to the cross_fold_validator, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Create a stratified K-Fold object\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Use cross_val_score with stratified K-Fold\n",
    "clf = LogisticRegression()\n",
    "scores = cross_val_score(pipeline, X, y, cv=stratified_kfold, scoring='accuracy')\n",
    "scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can apply a \"Leave One Out\" cross fold validator if we really want to.  Keep in mind that LOO can be computationally expensive for large datasets because it will train a new model for each sample in the dataset. It's generally used for small datasets or for cases where a high-variance estimate is acceptable.  Note, we'll only do this with a sample dataset, because it is computationally expensive to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a dataset\n",
    "X, y = make_classification(n_classes=2, class_sep=2,\n",
    "weights=[0.1, 0.9], n_informative=3, n_redundant=0,\n",
    "flip_y=0, n_features=20, n_clusters_per_class=1,\n",
    "n_samples=100, random_state=42)\n",
    "\n",
    "# Create a Leave-One-Out object\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Use cross_val_score with Leave-One-Out\n",
    "clf = LogisticRegression()\n",
    "scores = cross_val_score(clf, X, y, cv=loo, scoring='accuracy')\n",
    "\n",
    "print(f\"Mean Accuracy: {scores.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "\n",
    "1 - Use cross fold validation to evaluate average performance (F1-score) for the \"Wine\" dataset with folds from 3 - 10.  Plot both the average and std. err."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing\n",
    "\n",
    "One of the most common forms (but certainly not the only!) of data is the CSV file.  CSV files typically store tabular data where each line represents a row and columns are separated by a delimiter, which is commonly a comma.\n",
    "\n",
    "For example:\n",
    "```\n",
    "Name, Age, Gender\n",
    "Alice, 29, Female\n",
    "Bob, 35, Male\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Reading CSV Files\n",
    "You can use the `read_csv` function from the `pandas` library to read a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"data/fruits.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Different Delimiters\n",
    "Not all files use a comma as a delimiter. Some may use tabs, semicolons, or other characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tab delimited file\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"data/fruits.tsv\",delimiter=\"\\t\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A semi-colon delimited file\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"data/fruits-semicolon.csv\",delimiter=\";\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5\n",
    "\n",
    "Read the bank data in from your homework assignment!  Just use the \"bank\" data for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Inferencing\n",
    "Pandas tries to infer the data types of each column automatically, but sometimes it may not work as expected. You can explicitly specify types using the `dtype` parameter, and there is a special handling function for dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the fruits data\n",
    "fruits = pd.read_csv(\"data/fruits.csv\")\n",
    "print(fruits.dtypes)\n",
    "fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the date wasn't parsed, and that the `count` column should really be an integer\n",
    "fruits = pd.read_csv(\"data/fruits.csv\",dtype={'count':int},parse_dates=['Date'])\n",
    "print(fruits.dtypes)\n",
    "fruits\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that pandas treats \"null\" values as floats by default, so even though we told it to treat \"Count\" as an int, it still read it as a float!  We can take care of that by converting the column to \"Int64\" which pandas introduced to handle missing values in integer columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits['Count'] = fruits['Count'].astype('Int64')\n",
    "print(fruits.dtypes)\n",
    "fruits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning Issues\n",
    "Data often requires cleaning before it can be analyzed or used for building models. Two useful things to remember:\n",
    "\n",
    "1. **Dropping unnecessary columns**: \n",
    "    ```python\n",
    "    df.drop(['column_name'], axis=1, inplace=True)\n",
    "    ```\n",
    "   \n",
    "2. **Dropping duplicates**: \n",
    "    ```python\n",
    "    df.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that without \"inplace=True\" this won't modify the original data\n",
    "fruits.drop([\"Cost\"],axis=1,inplace=True)\n",
    "fruits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Handle Nulls and Missing Values\n",
    "Pandas will automatically detect common representations of missing values (`NA`, `NaN`, etc.). You can specify additional symbols using the `na_values` parameter.\n",
    "\n",
    "```python\n",
    "df = pd.read_csv('file.csv', na_values=['n/a', '??'])\n",
    "```\n",
    "\n",
    "Once we have a data frame with nulls, we have to figure out what to do with them.  Here are some options:\n",
    "\n",
    "#### Just drop NA's\n",
    "\n",
    "We can drop any rows that have NA's with: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits = pd.read_csv('data/fruits.csv')\n",
    "print(fruits)\n",
    "print(\"\\n\\n No nas\")\n",
    "# Note, we could use the 'inplace' parameter to alter the table.  We're not doing that here, though.\n",
    "fruits_nona = fruits.dropna()\n",
    "print(fruits_nona)\n",
    "\n",
    "# We can focus just on specific columns with the 'subset' parameter\n",
    "print(\"\\n\\n No nas in Cost column\")\n",
    "fruits_somena = fruits.dropna(subset=['Cost'])\n",
    "print(fruits_somena)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill NA's\n",
    "\n",
    "There are several ways to fill na's as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIll NA everywhere with zero\n",
    "fruits = pd.read_csv('data/fruits.csv')\n",
    "fruits.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NAs in just one column\n",
    "fruits = pd.read_csv('data/fruits.csv')\n",
    "fruits['Cost'].fillna(0,inplace=True)\n",
    "fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIll NA with a value based on other data\n",
    "fruits = pd.read_csv('data/fruits.csv')\n",
    "fruits['Category'].fillna(fruits.Category.mode()[0],inplace=True)\n",
    "fruits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can copy values forward and backward..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits = pd.read_csv('data/fruits.csv')\n",
    "# Forward fill (replace with the previous row's value; use 'bfill to copy from the next row's value)\n",
    "fruits.fillna(method='ffill', inplace=True)\n",
    "fruits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even perform linear interpolation, if our data is ordered..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we're interpolating based on surrounding values. Note that because we have two groups (fruits and vegetables) we'll interpolate these groups separately.\n",
    "\n",
    "fruits = pd.read_csv('data/fruits.csv')\n",
    "fruits['Cost'] = fruits.groupby(\"Category\")[\"Cost\"].apply(lambda x: x.interpolate(method='linear')).reset_index(drop=True)\n",
    "fruits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can simply use the `replace` method to replace arbitrary values anywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits = pd.read_csv('data/fruits.csv')\n",
    "fruits.replace({np.NaN: \"missing\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
