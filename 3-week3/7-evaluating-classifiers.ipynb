{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluation**\n",
    "\n",
    "A critical step in an ML workflow is evaluating your model. There are two important considerations when evaluating a classifier:\n",
    "\n",
    "1. How to sample your data for testing and\n",
    "2. What measurements to use.\n",
    "\n",
    "We'll talk about both of these issues below, after a brief discussion about the different types of errors and limitations of _accuracy_ as a metric for evaluating ML models. \n",
    "\n",
    "\n",
    "### **Training Error vs. Generalization Error**\n",
    "\n",
    "![](img/IST707-Week21.jpg)\n",
    "\n",
    "In machine learning, evaluating a model's performance involves understanding two key concepts: training error and generalization error. **Training error** refers to the error that the model makes on the training data, the same data it learns from. It's a measure of how well the model fits the training data. However, fitting the training data too closely can lead to overfitting, where the model captures noise along with the underlying data pattern. This is where **generalization error** comes into play. Generalization error measures how well a model performs on unseen data, that is, data not used during the training process. It's an indicator of how well the model has learned to generalize from the training data to broader, unseen instances. A model that performs well on training data but poorly on unseen data (high generalization error) is overfitted, whereas a model that achieves a balance, performing well both on training and unseen data, is considered well-generalized.\n",
    "\n",
    "#### Example\n",
    "Consider a model trained to predict house prices. If this model is trained on a specific dataset of house prices in a city and achieves very low error rates on this training set, it has a low training error. However, if this model, when used to predict prices of houses in a different city (unseen data), performs poorly, it has a high generalization error. The goal in model development is to minimize both training error and generalization error to create a model that is accurate and robust against unseen data.\n",
    "\n",
    "\n",
    "\n",
    "### **Accuracy is Not Always Enough**\n",
    "\n",
    "**Accuracy** is a commonly used metric in classification problems which calculates the proportion of correct predictions out of all predictions made. It's defined as the number of correct predictions (both true positives and true negatives) divided by the total number of predictions.\n",
    "\n",
    "However, accuracy can be misleading, especially in cases where the dataset is imbalanced, meaning there's a significant difference in the number of instances of the different classes.\n",
    "\n",
    "#### Limitations of Accuracy\n",
    "1. **Does Not Reflect Class Imbalance**: Accuracy does not take into account the imbalance in the distribution of the classes. In a highly imbalanced dataset, even a naive model predicting only the majority class can yield a high accuracy.\n",
    "2. **No Insight into Type of Errors**: Accuracy does not distinguish between the types of errors (false positives vs. false negatives), which can be critical in certain domains like medical diagnosis or fraud detection.\n",
    "\n",
    "#### Example: Breast Cancer Testing\n",
    "Consider a breast cancer screening test applied to 1000 individuals with an accuracy of 94.5%, and the base rate of breast cancer in this population is 5%.\n",
    "\n",
    "- Out of 1000 individuals, 50 (5%) have breast cancer, and 950 (95%) do not.\n",
    "- With an accuracy of 94.5%, the test correctly identifies 945 individuals (both cancer and non-cancer cases).\n",
    "\n",
    "Let's break down this accuracy:\n",
    "\n",
    "1. **True Positives (TP)**: These are individuals who have breast cancer and are correctly identified by the test as having it.\n",
    "2. **True Negatives (TN)**: These are individuals who do not have breast cancer and are correctly identified by the test as not having it.\n",
    "3. **False Positives (FP)**: These are individuals who do not have breast cancer but are incorrectly identified by the test as having it.\n",
    "4. **False Negatives (FN)**: These are individuals who have breast cancer but are incorrectly identified by the test as not having it.\n",
    "\n",
    "Given the accuracy and base rate, we can't directly deduce the exact number of TP, FP, TN, and FN, but we can infer the following:\n",
    "\n",
    "- If all 50 actual cancer cases are correctly identified, then we have 50 TPs. The remaining 895 correct predictions must be TNs (945 correct predictions minus 50 TPs). This would leave 55 incorrect predictions (1000 total minus 945 correct predictions), which would all have to be FPs since all the TPs are accounted for. This scenario implies a very high false positive rate.\n",
    "- Conversely, if some of the cancer cases are missed (FNs), the number of FPs would decrease, but this would increase the FN rate, which is also problematic.\n",
    "\n",
    "Thus, even with a high accuracy of 94.5%, the low base rate of cancer significantly affects the interpretation of the result. A high number of false positives or false negatives can still occur, which can be critical in a medical context. This example illustrates why accuracy alone, especially in the context of imbalanced datasets, might not be a sufficient metric for evaluating the performance of a diagnostic test. Other metrics like precision, recall, and the F1 score are crucial for a more complete evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Sampling Data**\n",
    "\n",
    "With a basic understanding of some of the issues underlying evaluation, we can now turn to the challenge of how to sample data for purposes of evaluation.\n",
    "\n",
    "## Train-Test Split\n",
    "\n",
    "* __Basic Idea__ :  Separate your data into two sets – one for “training” the model\\, and the other for “testing” the model\n",
    "* __Important Considerations__\n",
    "  * The training set must be  _completely independent_ and  _highly representative_ of the test set to get an unbiased estimate of performance\n",
    "  * Performance can vary depending on the split\\, and a bad\\-split could result in poorer performance\n",
    "* __Examples__\n",
    "  * Binary classification with a rare positive class \\(e\\.g\\.\\, disease detection where only 5% samples are positive\\)\n",
    "  * Time\\-series data where future data points are used in the training\\, and past data points in the training\n",
    "  * Using data from one geographic region in the training data and another region in the test set\n",
    "\n",
    "### Example\n",
    "\n",
    "The following shows how to use `train_test_split` in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load a built-in dataset (Iris dataset)\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Example without a fixed random seed\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Train a simple model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and calculate accuracy\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the train test split is different every time, and so you might get different results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for _ in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    results.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reproducibility, you can set a random seed to make sure the `random` engine produces the same split every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set a seed\n",
    "\n",
    "seed = 200\n",
    "\n",
    "results = []\n",
    "for _ in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    results.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Stratification\n",
    "\n",
    "Stratification is a technique used in data splitting, especially important in classification problems, to ensure that each subset of the data (such as training and testing sets) has a similar distribution of class labels as the original dataset. This is particularly crucial when dealing with imbalanced datasets, where one or more classes are underrepresented compared to others.\n",
    "\n",
    "When performing a train-test split without stratification in an imbalanced dataset, there's a risk that the distribution of classes in the training and testing sets will be significantly different from each other and from the original dataset. This can lead to biased or inaccurate models, as the model may not be trained or evaluated on a representative sample of data.\n",
    "\n",
    "Stratification addresses this issue by dividing the data in a way that maintains the same proportion of each class in both the training and testing sets as found in the original dataset. In Scikit-Learn's `train_test_split` function, this is achieved by setting the `stratify` parameter to the class labels. As a result, stratification leads to more reliable model evaluation and performance metrics, particularly in scenarios of class imbalance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "First, we're going to artificially unbalance scikit learn's built in breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Separate the majority and minority classes\n",
    "majority_class_X = X[y == 0]\n",
    "majority_class_y = y[y == 0]\n",
    "minority_class_X = X[y == 1]\n",
    "minority_class_y = y[y == 1]\n",
    "\n",
    "# Downsample the minority class\n",
    "minority_size = int(0.1 * len(majority_class_y))  # 10% of the majority class size\n",
    "downsampled_minority_X = minority_class_X[:minority_size]\n",
    "downsampled_minority_y = minority_class_y[:minority_size]\n",
    "\n",
    "# Combine the downsampled minority class with the majority class\n",
    "X_downsampled = np.concatenate([majority_class_X, downsampled_minority_X])\n",
    "y_downsampled = np.concatenate([majority_class_y, downsampled_minority_y])\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "X_downsampled, y_downsampled = shuffle(X_downsampled, y_downsampled, random_state=42)\n",
    "\n",
    "# Now X_downsampled and y_downsampled have the downsampled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now analyze results with the imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Function to perform train_test_split and evaluate the model\n",
    "def evaluate_model(X, y, stratify=None):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=stratify)\n",
    "    model = SGDClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Evaluating the model without stratification\n",
    "acc_without_stratification = [evaluate_model(X_downsampled, y_downsampled) for _ in range(200)]\n",
    "\n",
    "# Evaluating the model with stratification\n",
    "acc_with_stratification = [evaluate_model(X_downsampled, y_downsampled, stratify=y_downsampled) for _ in range(200)]\n",
    "\n",
    "# Display results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram for accuracies without stratification\n",
    "plt.subplot(1, 2, 1)  # 1 row, 2 columns, 1st subplot\n",
    "plt.hist(acc_without_stratification, bins=10, alpha=0.7, color='blue')\n",
    "plt.title('Accuracies without Stratification')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Histogram for accuracies with stratification\n",
    "plt.subplot(1, 2, 2)  # 1 row, 2 columns, 2nd subplot\n",
    "plt.hist(acc_with_stratification, bins=10, alpha=0.7, color='green')\n",
    "plt.title('Accuracies with Stratification')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()  # Adjusts the plots to fit into the figure cleanly\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the preceding example several times, you should find that the accuracies with stratification have lower overall variance than those models trained and tested without stratification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Cross-validation\n",
    "\n",
    "After observing that a single train-test split can lead to varying results, cross-validation emerges as a solution to gain a more stable and accurate estimate of a model's performance. In cross-validation, the dataset is divided into multiple smaller sets or \"folds\". The model is then trained and evaluated multiple times, with each fold getting a chance to serve as the test set while the remaining parts are used for training. The most common form of this technique is *k-fold cross-validation*, where the data is split into 'k' number of folds. For each iteration, a different fold is used for testing, and the rest for training. The final performance metric is typically the average of the model's performance across all folds.\n",
    "\n",
    "This process helps in mitigating the variability that comes with relying on a single split. It ensures that every data point is used for both training and testing, which makes the evaluation more reliable and less dependent on the particular way the data is split. By using cross-validation, you gain a better understanding of how the model is likely to perform on unseen data, making it a staple technique in the machine learning model evaluation process.\n",
    "\n",
    "* __K\\-Fold Cross validation:__\n",
    "  * Partition data into  _k_  subsets or \"folds\\.\"\n",
    "  * Train on  _k_ −1 of these folds and test on the remaining fold\n",
    "  * Repeat this process  _k_  times\\, average performance metrics\n",
    "* __Leave\\-one\\-out:__\n",
    "  * Extreme cross\\-validation \\- train on all available data\\, holding back just one case for testing\n",
    "  * Computationally very expensive\n",
    "* __Stratified K\\-Fold Cross\\-Validation:__\n",
    "  * Each fold is stratified\n",
    "* __Considerations__ :\n",
    "  * Not appropriate for time\\-series data \\(use time\\-series specific cross\\-validation\\)\n",
    "  * A greater number of folds increases available training data but\n",
    "  * Increases processing time and performance variance\n",
    "  * Reduces representativeness of test samples\n",
    "  * Data leakage if any preprocessing / feature selection is done after splitting but before training\\.  All such operations need to take place on the training set\\.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation code\n",
    "\n",
    "Scikit learn has a robust API for handling cross-validation, which offers a lot of flexibility.  The following offers a quick introduction.\n",
    "\n",
    "#### **Interlude: Preparing the data**\n",
    "\n",
    "For the following, we'll use a dataset of handwritten digits, commonly used to demonstrate ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(width=120)\n",
    "pp.pprint(mnist.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.keys()  # extra code – we only use data and target in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, data and target exist for most sklearn datasets.  Data is a matrix of features, and target is a vector of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist.data, mnist.target\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanted to see these as a DataFrame, you could do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(mnist.data)\n",
    "df[\"target\"] = mnist.target\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that's not really necessary here.  We'll just use these things as matrices and arrays.  Let's look at the shape here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X shape = {X.shape}\")\n",
    "print(f\"y shape = {y.shape}\")\n",
    "print(f\"Is number of rows equal? {X.shape[0] == y.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where does the 784 come from?  Read the docs above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "28 * 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one of these images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define function to plot a single digit\n",
    "def plot_digit(image_data):\n",
    "    image = image_data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "some_digit = X[0]\n",
    "plot_digit(some_digit)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 9))\n",
    "for idx, image_data in enumerate(X[:100]):\n",
    "    plt.subplot(10, 10, idx + 1)\n",
    "    plot_digit(image_data)\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the classfier\n",
    "\n",
    "Let's just go with predicting '5s'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Setting up the data here\n",
    "# Though it's not really necessary here (MINST variables are all roughly the same) it's often useful to z-score\n",
    "# your data.  Especially important for logistic regression\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Arbitratily taking the first 60000 rows here to reduce processing time\n",
    "X, y = X[:60000], y[:60000]\n",
    "\n",
    "y_5 = (y == '5')  # True for all 5s, False for all other digits\n",
    "\n",
    "# Save for later\n",
    "digit_data = X,y_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm going to use the SGDClassifier here - it's a lot like a logistic regression with an \"sag\" solver,\n",
    "# but is a lot faster with larger data sets\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "digit_clf = SGDClassifier(random_state=42)\n",
    "digit_clf.fit(X, y_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall we already checked that X[0] is a five\n",
    "some_digit = X[0]\n",
    "\n",
    "# Note that predict expect an array of values, so we tuck this into an array before testing.\n",
    "digit_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Using cross_val_score**\n",
    "\n",
    "`cross_val_score` is an easy one liner that handles most of the simple cases for cross validation.  In the following, we use three folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "sgd_clf = SGDClassifier()\n",
    "# Scikit learn makes it easy to use cross validation with simple measures\n",
    "# CV is the number of folds\n",
    "cross_val_score(sgd_clf, X, y_5, cv=3, scoring=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `cross_val_score` only reports one measure (passed as the 'scoring' parameter).  If you want to pass more than one measure, you can use the `cross_validate` method.  Note, we'll talk about the different measures later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scoring = {\n",
    "    'f1': 'f1_macro',\n",
    "    'precision': make_scorer(precision_score, average='macro'),\n",
    "    'recall': make_scorer(recall_score, average='macro')\n",
    "}\n",
    "\n",
    "# Using multiple metrics\n",
    "scores = cross_validate(sgd_clf, X, y_5, cv=3, scoring=scoring,return_train_score=True)\n",
    "for key, values in scores.items():\n",
    "    print(f\"{key}: {values.mean():.3f} (+/- {values.std() * 2:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **KFold sampler**\n",
    "\n",
    "If you want even more control over your training and testing, you can use the `KFold` class in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import accuracy_score\n",
    "kf = KFold(n_splits=3, random_state=42, shuffle=True)\n",
    "# Initialize Logistic Regression and StandardScaler\n",
    "clf = SGDClassifier()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Initialize list to store accuracy for each fold\n",
    "accuracy_list = []\n",
    "\n",
    "# Loop through each fold\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    # Split the data into current train and test set\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y_5[train_index], y_5[test_index]\n",
    "\n",
    "    # It's a good idea to use a fresh, untrained model each time you run on new data\n",
    "    # The \"clone\" command does that, but simplifies things by copying other parameters\n",
    "\n",
    "    clone_clf = clone(clf)\n",
    "\n",
    "    # Fit the model\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accuracy_list.append(acc)\n",
    "\n",
    "# Calculate and print the average accuracy\n",
    "average_accuracy = sum(accuracy_list) / len(accuracy_list)\n",
    "print(f'Average Accuracy: {average_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Stratified K-Fold Sampling**\n",
    "\n",
    "If we want to run a stratified K-Fold sampler, we can use the `StratifiedKFold` class.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "clf = SGDClassifier()\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits=3)  # add shuffle=True if the dataset is not\n",
    "                                       # already shuffled\n",
    "for train_index, test_index in skfolds.split(X, y_5):\n",
    "    clone_clf = clone(sgd_clf)\n",
    "    X_train_folds = X[train_index]\n",
    "    y_train_folds = y_5[train_index]\n",
    "    X_test_fold = X[test_index]\n",
    "    y_test_fold = y_5[test_index]\n",
    "\n",
    "    clone_clf.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_clf.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct / len(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Note that if we don't need fine-grained control, we can simply pass the sampler into the `cross_val_score` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Create a stratified K-Fold object\n",
    "stratified_kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Use cross_val_score with stratified K-Fold\n",
    "clf = SGDClassifier()\n",
    "scores = cross_val_score(clf, X, y_5, cv=stratified_kfold, scoring='accuracy')\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Working with pipelines**\n",
    "\n",
    "Note that combining scikit learn's cross validation routines with pipelines simplify the process of data preparation while avoiding data leakage.  \n",
    "\n",
    "As a working example, we'll use the `wine` dataset, which is focused on predicting the quality of wine based on it's chemical makeup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "wine_data = pd.read_csv(url, delimiter=\";\")\n",
    "wine_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll divide wines into `good` quality and `bad` quality by selecting all wines with a quality score of 7 or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary classification target variable\n",
    "wine_data['quality_label'] = wine_data['quality'].apply(lambda x: 1 if x >= 7 else 0)\n",
    "\n",
    "# Features and Target\n",
    "X = wine_data.drop(['quality', 'quality_label'], axis=1)\n",
    "y = wine_data['quality_label']\n",
    "\n",
    "wine_training_data = X,y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the wine dataset features are of different orders of magnitude, so we'll want to scale features before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline with StandardScaler and LogisticRegression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Step 1: Scale the data\n",
    "    ('classifier', LogisticRegression())  # Step 2: Train a logistic regression model\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the `Pipeline` class follows the estimator API, so all we have to do is pass it into `cross_val_score`.  This will take care of running scaling separately on each fold, hence avoiding any data leakage problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline with StandardScaler and LogisticRegression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Step 1: Scale the data\n",
    "    ('classifier', LogisticRegression())  # Step 2: Train a logistic regression model\n",
    "])\n",
    "# Use cross_val_score to get the scores for each fold\n",
    "scores = cross_val_score(pipeline, X, y, cv=5)\n",
    "\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean cross-validation score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Leave One Out evaluation**\n",
    "\n",
    "Finally, sklearn provide a `LeaveOneOut` sampler. Keep in mind that LOO can be computationally expensive for large datasets because it will train a new model for each sample in the dataset. It's generally used for small datasets or for cases where a high-variance estimate is acceptable.  Note, we'll only do this with a sample dataset, because it is computationally expensive to run.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a dataset\n",
    "X, y = make_classification(n_classes=2, class_sep=2,\n",
    "weights=[0.1, 0.9], n_informative=3, n_redundant=0,\n",
    "flip_y=0, n_features=20, n_clusters_per_class=1,\n",
    "n_samples=100, random_state=42)\n",
    "\n",
    "# Create a Leave-One-Out object\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Use cross_val_score with Leave-One-Out\n",
    "clf = LogisticRegression()\n",
    "scores = cross_val_score(clf, X, y, cv=loo, scoring='accuracy')\n",
    "\n",
    "print(f\"Mean Accuracy: {scores.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Metrics**\n",
    "\n",
    "As we've discussed, accuracy is limited in several ways.  There are many other means for evaluating performance, and what is most meaningful depends on the domain problem.  Scikit learn has a robust set of metrics, and I encourage you to read through the documentation hosted at https://scikit-learn.org/stable/modules/model_evaluation.html.\n",
    "\n",
    "In the following, we'll cover some of the more common approaches as well as there implementation in sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Confusion Matrices**\n",
    "\n",
    "A  _confusion matrix_  is a table layout that allows visualization of the performance of an algorithm.  Though not as convenient as a single number, confusion matrices provides a great deal of information, especially in the case of binary classifiers.  The cells of a confusion matrix include the following values:\n",
    "\n",
    "__True Positives \\(TP\\)__ : These are the correctly predicted positive observations\\.\n",
    "\n",
    "__True Negatives \\(TN\\)__ : These are the correctly predicted negative observations\\.\n",
    "\n",
    "__False Positives \\(FP\\)__ : Incorrectly predicted positive observations \\(Type I error\\)\\.\n",
    "\n",
    "__False Negatives \\(FN\\)__ : Incorrectly predicted negative observations \\(Type II error\\)\\.\n",
    "\n",
    "**Example**\n",
    "\n",
    "Considers and online store that sells video games, and a model that predicts whether a visitor will buy a game or not.\n",
    "\n",
    "|   | predictions |  |  |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| Ground Truth | buy_game = yes | buy_game = no | total |\n",
    "| buy_game = yes | 6700 (TP) | 300 (FN) | 7000 |\n",
    "| buy_game = no | 900 (FP) | 100 (TN) | 1000 |\n",
    "| total | 7600 | 400 | 8000 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sklearn, we can easily run a confusion matrix as follows (using our digit data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You've hopefully already found the sklearn metrics library\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X = digit_data[0]\n",
    "y = digit_data[1]\n",
    "\n",
    "\n",
    "clf = SGDClassifier()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test) \n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Precision, Recall, and F1 Score**\n",
    "\n",
    "Precision, Recall, and the F1-Score are alternative metrics used to evaluate the performance of classification models, especially in scenarios where classes are imbalanced. Understanding these metrics is key to interpreting the effectiveness of a model beyond just accuracy.\n",
    "\n",
    "### Precision\n",
    "- **What It Measures**: Precision quantifies the accuracy of the model in predicting positive instances. It's the ratio of true positives (correctly predicted positives) to the total number of predicted positives (both true positives and false positives).\n",
    "- **Formula**: Precision = True Positives / (True Positives + False Positives)\n",
    "- **Interpretation**: A high precision indicates that the model is reliable in its positive predictions, but it doesn’t tell us how many actual positives were missed.\n",
    "\n",
    "### Recall (Sensitivity)\n",
    "- **What It Measures**: Recall measures the model's ability to detect positive instances among all actual positives. It's the ratio of true positives to the actual positives in the data (both true positives and false negatives).\n",
    "- **Formula**: Recall = True Positives / (True Positives + False Negatives)\n",
    "- **Interpretation**: High recall means that the model is good at detecting positive instances, but it may also be including some false positives.\n",
    "\n",
    "### F1-Score\n",
    "- **What It Measures**: The F1-Score is the harmonic mean of precision and recall. It provides a single score that balances both the concerns of precision and recall in one number.\n",
    "- **Formula**: F1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "- **Interpretation**: A high F1-Score suggests a balanced trade-off between precision and recall. It is particularly useful when you seek a balance between these two metrics and when the class distribution is imbalanced.\n",
    "\n",
    "![](img/IST707-Week223.png)\n",
    "\n",
    "\\* By Walber \\- Own work\\, CC BY\\-SA 4\\.0\\, https://commons\\.wikimedia\\.org/w/index\\.php?curid=36926283\n",
    "\n",
    "#### Example\n",
    "\n",
    "|   | predictions |  |  |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| Ground Truth | buy_game = yes | buy_game = no | total |\n",
    "| buy_game = yes | 6700 (TP) | 300 (FN) | 7000 |\n",
    "| buy_game = no | 900 (FP) | 100 (TN) | 1000 |\n",
    "| total | 7600 | 400 | 8000 |\n",
    "\n",
    "Precision: 6700/7600 = \\.88\n",
    "\n",
    "Recall: 6700 / 7000 = \\.96\n",
    "\n",
    "F1\\-Score:  2\\*\\(\\.96\\+\\.88\\)/\\(\\.96\\*\\.88\\)=\\.92\n",
    "\n",
    "Precision: 100/400 = \\.25\n",
    "\n",
    "Recall: 100 / 1000 = \\.1\n",
    "\n",
    "F1\\-Score:  2\\*\\(\\.25\\+\\.1\\)/\\(\\.25\\*\\.1\\)=\\.14\n",
    "\n",
    "__Average F1 across classes = \\.53__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Calculating Precision, Recall and F1**\n",
    "\n",
    "Continuing with our digit dataset, sklearn provides metrics that make it easy to calculate precision and recall, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision_score(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same computation, using the confusion matrix above\n",
    "# TP / (FP + TP)\n",
    "cm[1, 1] / (cm[0, 1] + cm[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TP / (FN + TP)\n",
    "cm[1, 1] / (cm[1, 0] + cm[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating by hand\n",
    "cm[1, 1] / (cm[1, 1] + (cm[1, 0] + cm[0, 1]) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ROC Curves**\n",
    "\n",
    "\n",
    "Many machine learning classifiers, especially binary classifiers, work by computing a decision score or probability for each instance. This score indicates the likelihood of an instance belonging to a particular class. To make a final classification decision (e.g., class 0 or class 1), a threshold is set. Instances with scores above this threshold are assigned to one class, while those below are assigned to the other.\n",
    "\n",
    "For example, in a logistic regression classifier, a decision score is calculated for each instance, often in the form of a probability. By default, a threshold of 0.5 is typically used: instances with probabilities above 0.5 are classified as the positive class, and those below as the negative class.\n",
    "\n",
    "### The Precision-Recall Tradeoff\n",
    "The choice of threshold has a direct impact on precision and recall, leading to their tradeoff:\n",
    "\n",
    "- **Lowering the Threshold**: This increases recall but can decrease precision. By lowering the threshold, more instances are classified as positive, which means more actual positives are correctly identified (higher recall). However, this also leads to more false positives (lower precision).\n",
    "  \n",
    "- **Raising the Threshold**: Conversely, increasing the threshold boosts precision but can lower recall. A higher threshold means that only instances with a high likelihood are classified as positive, leading to fewer false positives (higher precision). However, this might result in missing out on actual positives (lower recall).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"decision_function\" method returns the raw value, of the predictor, which is then \n",
    "# thresholded to achieve an outcome\n",
    "\n",
    "y_scores = digit_clf.decision_function([some_digit])\n",
    "y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "# We can plug this directly into cross_val_predict to get the scores across all of our data\n",
    "y_scores = cross_val_predict(digit_clf, X, y, cv=3,\n",
    "                             method=\"decision_function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit learn gives us a really nice way to look at this!\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "threshold = 3000\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n",
    "plt.figure(figsize=(8, 4))  # extra code – it's not needed, just formatting\n",
    "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "plt.vlines(threshold, 0, 1.0, \"k\", \"dotted\", label=\"threshold\")\n",
    "\n",
    "idx = (thresholds >= threshold).argmax()  # first index ≥ threshold\n",
    "plt.plot(thresholds[idx], precisions[idx], \"bo\")\n",
    "plt.plot(thresholds[idx], recalls[idx], \"go\")\n",
    "plt.axis([-50000, 50000, 0, 1])\n",
    "plt.grid()\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.legend(loc=\"center right\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can graph these two together:\n",
    "\n",
    "import matplotlib.patches as patches  # extra code – for the curved arrow\n",
    "\n",
    "plt.figure(figsize=(6, 5))  # extra code – not needed, just formatting\n",
    "\n",
    "plt.plot(recalls, precisions, linewidth=2, label=\"Precision/Recall curve\")\n",
    "\n",
    "plt.plot([recalls[idx], recalls[idx]], [0., precisions[idx]], \"k:\")\n",
    "plt.plot([0.0, recalls[idx]], [precisions[idx], precisions[idx]], \"k:\")\n",
    "plt.plot([recalls[idx]], [precisions[idx]], \"ko\",\n",
    "         label=\"Point at threshold 3,000\")\n",
    "plt.gca().add_patch(patches.FancyArrowPatch(\n",
    "    (0.79, 0.60), (0.61, 0.78),\n",
    "    connectionstyle=\"arc3,rad=.2\",\n",
    "    arrowstyle=\"Simple, tail_width=1.5, head_width=8, head_length=10\",\n",
    "    color=\"#444444\"))\n",
    "plt.text(0.56, 0.62, \"Higher\\nthreshold\", color=\"#333333\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.grid()\n",
    "plt.legend(loc=\"lower left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this analysis, we can arbitrarily obtain a threshold to achieve a given level of precision or recall:\n",
    "\n",
    "idx_for_90_precision = (precisions >= 0.90).argmax()\n",
    "threshold_for_90_precision = thresholds[idx_for_90_precision]\n",
    "threshold_for_90_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Balancing the Tradeoff\n",
    "- **Context-Dependent**: The optimal balance between precision and recall is highly dependent on the specific context or application. For instance, in spam email detection (where false positives are more tolerable than false negatives), a lower threshold might be preferable. In contrast, for medical diagnostics (where missing a positive diagnosis can be critical), a higher threshold might be chosen to ensure high recall.\n",
    "  \n",
    "- **Adjusting the Threshold**: Some classifiers allow manual adjustment of the decision threshold. Experimenting with different thresholds can help in finding the balance that best suits the specific needs and priorities of the task at hand.\n",
    "\n",
    "### ROC and AUC\n",
    "\n",
    "Receiver Operating Characteristic (ROC) curves and the Area Under the Curve (AUC), are tools that examine the tradeoff between precision and recall. These are particularly useful for evaluating classification models in the presence of class imbalance or when dealing with probabilistic predictions.\n",
    "\n",
    "#### ROC Curves\n",
    "- **What It Is**: Like the precision-recall curve above, the ROC curve is a graphical representation of a classifier's performance across all possible thresholds. It plots two parameters: \n",
    "   - **True Positive Rate (TPR)**, also known as Recall, on the y-axis.\n",
    "   - **False Positive Rate (FPR)** on the x-axis.\n",
    "- **TPR vs. FPR**: TPR (Recall) is calculated as True Positives / (True Positives + False Negatives), and FPR is calculated as False Positives / (False Positives + True Negatives).\n",
    "- **Interpreting the Curve**: The ROC curve shows the tradeoff between sensitivity (or TPR) and specificity (1 - FPR). A higher curve indicates a better performance. An ideal classifier would have a curve that goes straight up the y-axis and then along the x-axis.\n",
    "\n",
    "Sklearn provides tools to simplify visualization of ROC Curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SciKit learn also gives us ROC curves for free\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n",
    "\n",
    "idx_for_threshold_at_90 = (thresholds <= threshold_for_90_precision).argmax()\n",
    "tpr_90, fpr_90 = tpr[idx_for_threshold_at_90], fpr[idx_for_threshold_at_90]\n",
    "\n",
    "\n",
    "plt.plot(fpr, tpr, linewidth=2, label=\"ROC curve\")\n",
    "plt.plot([0, 1], [0, 1], 'k:', label=\"Random classifier's ROC curve\")\n",
    "plt.plot([fpr_90], [tpr_90], \"ko\", label=\"Threshold for 90% precision\")\n",
    "\n",
    "# just beautifies the figure\n",
    "plt.gca().add_patch(patches.FancyArrowPatch(\n",
    "    (0.20, 0.89), (0.07, 0.70),\n",
    "    connectionstyle=\"arc3,rad=.4\",\n",
    "    arrowstyle=\"Simple, tail_width=1.5, head_width=8, head_length=10\",\n",
    "    color=\"#444444\"))\n",
    "plt.text(0.12, 0.71, \"Higher\\nthreshold\", color=\"#333333\")\n",
    "plt.xlabel('False Positive Rate (Fall-Out)')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.grid()\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.legend(loc=\"lower right\", fontsize=13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the above example, we have enough data that the ROC curve appears to be smooth, but this is not always the case.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "n_samples = 100\n",
    "\n",
    "# Generate true labels (40% of them are positive)\n",
    "y_true = np.random.choice([0, 1], size=n_samples, p=[0.8, 0.2])\n",
    "\n",
    "# Generate predicted probabilities\n",
    "y_score = np.linspace(0, 1, n_samples)\n",
    "np.random.shuffle(y_score)\n",
    "\n",
    "# Introduce some noise to make it more realistic\n",
    "y_score = y_score + np.random.normal(0, 0.1, n_samples)\n",
    "y_score = np.clip(y_score, 0, 1)\n",
    "\n",
    "# Compute ROC curve and AUC score\n",
    "fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "roc_auc = roc_auc_score(y_true, y_score)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### AUC (Area Under the ROC Curve)\n",
    "AUC provides a single numeric metric to summarize the ROC curve. It represents the probability that the classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one.\n",
    "- **Interpretation**: \n",
    "   - An AUC of 1 indicates a perfect classifier.\n",
    "   - An AUC of 0.5 suggests a no-skill classifier, equivalent to random guessing.\n",
    "   - AUC values between 0.5 and 1 indicate different levels of classifier performance, with higher values signifying better classification.\n",
    "\n",
    "#### Calculating the AUC\n",
    "\n",
    "For probabilistic classifiers (not all classifiers are probabilistic!), you can use sklearn to calculate the AUC as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = digit_data[0]\n",
    "y = digit_data[1]\n",
    "\n",
    "\n",
    "clf = LogisticRegression()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Step 1: Scale the data; important for Logistic regression\n",
    "    ('classifier', LogisticRegression(max_iter=5000))  # Step 2: Train a logistic regression model\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train,y_train)\n",
    "\n",
    "# Get the fitted classifier\n",
    "fitted_clf = pipeline.named_steps['classifier']\n",
    "y_pred_prob = fitted_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculating AUC\n",
    "auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"AUC:\", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Trapezoidal Rule\n",
    "\n",
    "The AUC can also be computed manually using the trapezoidal rule, which is a method for estimating the area under a curve. This is particularly useful when you have the ROC curve points.\n",
    "\n",
    "For example, if `(x[i], y[i])` and `(x[i+1], y[i+1])` are two consecutive points on the ROC curve, the area of the trapezoid between these points is given by:\n",
    "\n",
    "$$\\text{Area} = \\frac{1}{2} \\times (x[i+1] - x[i]) \\times (y[i] + y[i+1])$$\n",
    "\n",
    "To calculate the total AUC, sum up the areas of all such trapezoids formed by the consecutive points.\n",
    "\n",
    "**Code Example**:\n",
    "First, you need to obtain the TPR (True Positive Rate) and FPR (False Positive Rate) values at various thresholds, which can be done using `roc_curve` from Scikit-Learn. Then, apply the trapezoidal rule to these points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Calculate FPR, TPR, and thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Apply the trapezoidal rule to estimate AUC\n",
    "auc_manual = np.trapz(tpr, fpr)\n",
    "print(\"Manually Calculated AUC:\", auc_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation**:\n",
    "- In the Scikit-Learn example, `predict_proba` is used to get the probability scores required for AUC calculation.\n",
    "- In the manual calculation, `roc_curve` provides the FPR and TPR at various threshold levels, and `np.trapz` from NumPy applies the trapezoidal rule to these points to estimate the AUC.\n",
    "\n",
    "Both methods give you the same (or very close) AUC value, illustrating the model's ability to distinguish between the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "ROC curves and AUC are powerful tools for evaluating and comparing classifiers, especially in scenarios where choosing an optimal threshold is challenging or when dealing with probabilistic predictions. They provide a comprehensive view of model performance across various thresholds and are robust against imbalances in the class distribution.\n",
    "\n",
    "- **Threshold Independence**: Unlike precision and recall which depend on a specific threshold, ROC and AUC provide a threshold-independent measure of model performance. This is particularly useful when comparing different models.\n",
    "- **Class Imbalance**: ROC curves are less sensitive to class imbalance compared to precision-recall curves. Even with an imbalanced dataset, the ROC curve can provide a meaningful representation of a model's ability to distinguish between classes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
