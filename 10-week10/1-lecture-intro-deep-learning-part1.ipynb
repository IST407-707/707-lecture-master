{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning\n",
    "\n",
    "Deep Learning is a machine learning technique that teaches computers to do what comes naturally to humans: learn by example. It's a key technology behind driverless cars, enabling them to recognize a stop sign, or distinguishing a pedestrian from a lamppost. It's the voice of your virtual assistant that answers your questions, the recommendation engines that suggest your next favorite flick, or the powerful search engines that predict what you might be looking for.\n",
    "\n",
    "Deep Learning is a part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised, or unsupervised.\n",
    "\n",
    "Deep Learning models are loosely modeled on the information processing patterns found in the human brain. Just as we use our brains to recognize patterns and classify different types of information, deep learning algorithms can be taught to accomplish the same tasks for machines.\n",
    "\n",
    "\n",
    "### 1.1 Biological Neurons\n",
    "\n",
    "**Understanding Biological Neurons:**\n",
    "\n",
    "Before we jump into the intricacies of artificial neural networks, let's take a moment to appreciate the biological wonders that inspired them – the neurons in the human brain.\n",
    "\n",
    "- **The Human Brain:** Our brains are composed of approximately 86 billion neurons. These neurons are the workhorses of our thought processes, handling everything from the most automatic reflexes to complex decision-making.\n",
    "\n",
    "- **Structure of a Neuron:** A neuron has a cell body with dendrites and an axon. The dendrites act like input channels, receiving signals from other neurons, while the axon acts like an output channel, sending signals to other neurons.\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "\n",
    "![Neuron image](assets/neuron.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "- **Synaptic Transmission:** When a neuron receives a sufficient number of signals from its dendrites, it becomes activated and passes along an electrochemical signal through its axon. This process is known as synaptic transmission, where neurotransmitters are released into the synapse, the junction between neurons.\n",
    "\n",
    "- **Network of Neurons:** Each neuron can form thousands of links with other neurons, creating an incredibly complex network that is capable of a vast range of functions, from basic reflexes to advanced cognitive abilities.\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "\n",
    "![Neuron image](assets/brain_network.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "**From Biology to Artificial Intelligence:**\n",
    "\n",
    "The neural networks in Deep Learning draw inspiration from this biological setup. The idea is to create artificial \"neurons\" that can connect and communicate with one another, forming networks that can learn to perform tasks without being explicitly programmed to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Single Perceptron\n",
    "\n",
    "**The Perceptron Model:**\n",
    "\n",
    "The perceptron is the simplest type of artificial neural network and can be considered the building block for more complex networks. It was developed in the 1950s and 1960s by the scientist Frank Rosenblatt. A perceptron mimics a single neuron with multiple input signals and one output signal, and it operates on a simple principle: it computes a weighted sum of the inputs, adds a bias, and then passes this sum through an activation function to produce an output.\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "\n",
    "![Perceptron image](assets/perceptron.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "**How a Perceptron Learns:**\n",
    "\n",
    "1. **Weights and Bias:**\n",
    "   - **Weights:** In a perceptron, every input has an associated weight that represents its relative importance. These weights are adjustable parameters that the learning algorithm will learn.\n",
    "   - **Bias:** The bias is an additional parameter that allows the model to shift the activation function to the left or right, which helps the perceptron make better decisions.\n",
    "\n",
    "2.  **Activation Function:**\n",
    "    - Once the weighted sum of the inputs is computed, an activation function is applied to this sum. The simplest form of activation function is a step function that outputs a 1 if the sum is above a certain threshold and 0 otherwise.\n",
    "\n",
    "3. **Learning Process:**\n",
    "   - During the training process, the perceptron receives inputs with known outputs. It makes predictions by computing the weighted sum and applying the activation function. If the prediction is wrong, the weights and bias are updated in such a way as to reduce the error in prediction. This process is repeated for many iterations or until the model performs well.\n",
    "\n",
    "**Linear Separability and Limitations:**\n",
    "\n",
    "A single perceptron can only learn linearly separable patterns. Linear separability means that there exists a hyperplane that can separate the classes of data points. For instance, if you were to plot two categories of data points on a graph, a single line (or hyperplane in higher dimensions) should be able to separate them. This is a significant limitation because many real-world problems are not linearly separable.\n",
    "\n",
    "**Hands-On Python Example Using SKLearn's Perceptron:**\n",
    "\n",
    "Let's implement a simple perceptron using Python and SKLearn to see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a synthetic dataset for demonstration\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "\n",
    "# Visualizing the dataset\n",
    "plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], s=10, label='Class 0')\n",
    "plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], s=10, label='Class 1')\n",
    "plt.title(\"Synthetic Binary Classification Dataset\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Perceptron\n",
    "clf = Perceptron(max_iter=1000, tol=1e-3, random_state=0)\n",
    "\n",
    "# Train the perceptron\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Perceptron classification accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Plot the decision boundary\n",
    "# (This will be a simplified version for illustration purposes)\n",
    "import numpy as np\n",
    "\n",
    "# Set min and max values and give it some padding\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "# Generate a grid of points with distance h between them\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "\n",
    "# Predict the function value for the whole grid\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the contour and training examples\n",
    "plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', s=20)\n",
    "plt.title(\"Perceptron Decision Boundary on the Training Set\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The publication of the book \"Perceptrons\" by Marvin Minsky and Seymour Papert in 1969 pointed out that perceptrons are incapable of solving some simple problems - in particular, a simple perceptron cannot not solve the \"XOR\" problem.  This led to a temporary decline in the popularity and funding of neural network research. However, the advent of MLPs and backpropagation brought about a resurgence in the field, leading to the deep learning revolution we are witnessing today.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Multilayer Perceptron (MLP)\n",
    "\n",
    "**The Architecture of MLPs:**\n",
    "\n",
    "Multilayer Perceptrons (MLPs), also known as feedforward neural networks, are a class of deep neural networks with one or more hidden layers between the input and output layers. Each layer is made up of a number of interconnected nodes or neurons, and each connection has a weight associated with it.\n",
    "\n",
    "- **Input Layer:** The first layer that receives the input signal to be processed.\n",
    "- **Hidden Layers:** One or more layers where the actual processing is done via a system of weighted connections. The number of hidden layers and the number of neurons in each hidden layer are parameters that can be adjusted depending on the complexity of the task.\n",
    "- **Output Layer:** The final layer that produces the output of the network.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "\n",
    "![MLP](assets/mlp.png)\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Gradient Descent:**\n",
    "\n",
    "Gradient Descent is the optimization algorithm used to minimize the cost function, which is a measure of how far off the network's predictions are from the actual values. The process involves:\n",
    "\n",
    "- **Cost Function:** A function (like Mean Squared Error for regression tasks) that measures the difference between the predicted values by the network and the true values.\n",
    "- **Learning Rate:** A hyperparameter that determines the size of the steps we take to reach a (local) minimum.\n",
    "- **Update Rule:** A rule to adjust the weights of the network in the opposite direction of the gradient of the cost function with respect to the weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hands-on Exercise with Gradient Descent**\n",
    "\n",
    "A useful way to understand gradient descent is to start with a known error function.  Imagine that we have a quadratic error function such as $f(x) = x^2$.  The gradient of this error function is just the first derivative of the function $f(x)$.  Using this, finish the following code to calculate the gradient.  After you've done that, try manipulating the learning rate.  What happens?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function and Derivative\n",
    "f = lambda x: x ** 2\n",
    "df =  #??? What is the derivative?\n",
    "\n",
    "# Gradient Descent\n",
    "def gradient_descent(starting_x, learning_rate, n_iter=10):\n",
    "    x_path = [starting_x] # x_path is an array that stores our successive \"guesses\".  \n",
    "    for _ in range(n_iter):\n",
    "        gradient = df(x_path[-1]) #We are applying the gradient function (dervative) evaluated at our last guess\n",
    "        new_x = #what goes here?\n",
    "        x_path.append(new_x)\n",
    "    return x_path\n",
    "\n",
    "# Parameters\n",
    "starting_x = 9\n",
    "learning_rate = .1\n",
    "n_iter = 20\n",
    "\n",
    "# Run Gradient Descent\n",
    "x_path = gradient_descent(starting_x, learning_rate, n_iter)\n",
    "\n",
    "# Plotting\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = f(x)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x, y, label='f(x) = x^2')\n",
    "\n",
    "# Plot the steps with lines connecting them\n",
    "for i in range(1, len(x_path)):\n",
    "    plt.scatter(x_path[i-1:i+1], f(np.array(x_path[i-1:i+1])), color='red')\n",
    "    plt.plot(x_path[i-1:i+1], f(np.array(x_path[i-1:i+1])), color='red')\n",
    "\n",
    "plt.title('Gradient Descent Visualization with Paths')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, consider the the function $f(x) = (x-2)^4$.  Complete the code and examine the process of gradient decent.  What do you notice?  How many iterations do you need to find the minimum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function and Derivative\n",
    "f = lambda x: (x - 2) ** 4\n",
    "df = # What's the derivative?\n",
    "\n",
    "# Gradient Descent\n",
    "def gradient_descent(starting_x, learning_rate, n_iter=10):\n",
    "    x_path = [starting_x]\n",
    "    for _ in range(n_iter):\n",
    "        gradient = df(x_path[-1])\n",
    "        new_x = #what goes here?\n",
    "        x_path.append(new_x)\n",
    "        \n",
    "    return x_path\n",
    "\n",
    "# Parameters\n",
    "starting_x = 3  \n",
    "learning_rate = .1  \n",
    "n_iter = 100\n",
    "\n",
    "# Run Gradient Descent\n",
    "x_path = gradient_descent(starting_x, learning_rate, n_iter)\n",
    "\n",
    "# Plotting\n",
    "x = np.linspace(1, 3, 1000)  # Range includes the minimum and starting point\n",
    "y = f(x)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x, y, label='f(x) = (x - 2)^4')\n",
    "\n",
    "# Plot the steps with lines connecting them\n",
    "for i in range(1, len(x_path)):\n",
    "    plt.scatter(x_path[i-1:i+1], f(np.array(x_path[i-1:i+1])), color='red')\n",
    "    plt.plot(x_path[i-1:i+1], f(np.array(x_path[i-1:i+1])), color='red')\n",
    "\n",
    "plt.title('Gradient Descent Visualization')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Backpropagation:**\n",
    "\n",
    "In the real-world application of neural networks, we don't have an explicit formula for the loss function like $f(x) = x^2$. Instead, the loss function is determined by the architecture of the neural network, the weights of the connections, the biases, the choice of activation functions, and the dataset being used.\n",
    "\n",
    "Backpropagation is the central mechanism by which neural networks calculate and follow the gradient during the learning process. It involves:\n",
    "\n",
    "1. **Forward Pass:**\n",
    "   - Data is passed through the network (forward pass), and a prediction is made.\n",
    "   - The loss function is evaluated using the predicted output and the true labels.\n",
    "\n",
    "2. **Backpropagation:**\n",
    "   - Backpropagation is then used to calculate the gradients of the loss function with respect to each weight in the network.\n",
    "   - Even though we don't have a simple formula for the loss as in $f(x) = x^2$, we know that the loss is a function of the weights and biases in the network, and we can compute its gradient using the chain rule of calculus.\n",
    "\n",
    "3. **Chain Rule:**\n",
    "   - The chain rule is applied recursively from the output layer back to the input layer, hence the name \"backpropagation\".\n",
    "   - The derivative of the loss function with respect to a weight is calculated by considering how a change in that weight affects the loss.\n",
    "\n",
    "4. **Computing Derivatives:**\n",
    "   - Even though we don't have an explicit formula for the loss, we can calculate the derivative of the loss with respect to each parameter using automatic differentiation tools that are built into deep learning frameworks like TensorFlow and PyTorch.\n",
    "   - These tools allow us to compute the gradient of the loss with respect to all the weights without manually deriving the gradient functions.\n",
    "\n",
    "5. **Gradient Descent:**\n",
    "   - With the gradients calculated, we can perform a gradient descent step to update the weights in the direction that will most reduce the loss.\n",
    "   - This process is repeated for many iterations over the training dataset, progressively reducing the loss and improving the model's predictions.\n",
    "\n",
    "Thus, while we may not have an explicit mathematical function for the loss in neural networks, the process of backpropagation allows us to compute the necessary gradients to perform gradient descent. This is why neural networks are so powerful—they can learn to minimize loss even when the relationship between parameters and loss is highly complex and not explicitly defined.\n",
    "\n",
    "**Activation Functions:**\n",
    "\n",
    "Activation functions play a crucial role in neural networks by introducing non-linear properties to the model. Without non-linearity, no matter how many layers the neural network has, it would behave just like a single-layer perceptron, which can only solve linearly separable problems. \n",
    "\n",
    "1. **Linear Limitations:**\n",
    "   - If a neural network used only linear transformations (i.e., without any activation functions or with only linear activation functions), the entire network would be equivalent to a single linear transformation. This is because linear transformations can be composed; that is, the combination of two linear transformations is itself a linear transformation.\n",
    "   - This compounded linear transformation would not be able to capture the complex relationships in data that are non-linear in nature.\n",
    "\n",
    "2. **Stacking Layers:**\n",
    "   - The key idea behind deep learning is to process data through multiple layers where each layer's output is fed into the next layer. The complexity and capability of the model grow with each layer, allowing it to learn a hierarchy of features.\n",
    "   - Without activation functions, adding more layers wouldn't increase the network's hypothesis space beyond that of a single layer. The additional layers would offer no new capability because the output of the network would still be a linear function of the input.\n",
    "\n",
    "3. **Why Non-Linearity Matters:**\n",
    "   - Most real-world data cannot be explained by a linear model; complexities and patterns are typically non-linear.\n",
    "   - Non-linear activation functions allow neural networks to compute arbitrarily complex functions. By using non-linear transformations, a neural network can learn to approximate non-linear relationships between inputs and outputs, which is essential for tasks like image and speech recognition, natural language processing, and many others.\n",
    "\n",
    "4. **Universal Approximation Theorem:**\n",
    "   - The Universal Approximation Theorem states that a feedforward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of $\\mathbb{R}^n$, under mild assumptions on the activation function. The activation function must be non-linear for this to hold true.\n",
    "   - This means that theoretically, neural networks can learn to represent a wide variety of functions given enough neurons and the right non-linear activation function.\n",
    "\n",
    "**Common activation functions**\n",
    "\n",
    "- **Heavyside**  A simple step function that returns a 1 if the input is greater than or equal to a threshold, and 0 otherwise.\n",
    "- **Sigmoid:** It squashes the input values into a range between 0 and 1. It's often used for the output layer in binary classification problems.\n",
    "- **Tanh** The hyberbolic tangent, which is effectively a scaled version of the sigmoid.  A key different is that it returns a value between -1 and 1, rather than 0 or 1.\n",
    "- **ReLU (Rectified Linear Unit):** It outputs the input directly if it is positive, otherwise, it outputs zero. It has become the default activation function for many types of neural networks because it allows for faster training.\n",
    "\n",
    "**Practical Implications**\n",
    "\n",
    "1. **Breaking Symmetry:**\n",
    "   - During training, if two neurons have the same input and activation function, they would always update the same way. Non-linear activation functions help to break this symmetry.\n",
    "\n",
    "2. **Vanishing and Exploding Gradients:**\n",
    "   - While activation functions are necessary for neural networks to model complex data, the choice of function can affect training dynamics. For example, sigmoid or tanh can lead to vanishing gradients, making training deep networks difficult.\n",
    "\n",
    "3. **ReLU and Variants:**\n",
    "   - To address some of these training issues, functions like ReLU (Rectified Linear Unit) and its variants (Leaky ReLU, Parametric ReLU, etc.) have become popular. ReLU is piecewise linear but non-linear overall because it introduces a point of non-differentiability at 0.\n",
    "\n",
    "4. **Computational Efficiency:**\n",
    "   - Some non-linear functions, like ReLU, are also computationally efficient, which can significantly speed up training without sacrificing the ability to represent non-linear functions.\n",
    "\n",
    "In conclusion, activation functions allow neural networks to map complex and non-linear hypotheses. Without them, deep learning would not be able to achieve the impressive results it does on a vast array of tasks that involve understanding and processing data with intricate, non-linear relationships.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def derivative(f, z, eps=0.000001):\n",
    "    return (f(z + eps) - f(z - eps))/(2 * eps)\n",
    "\n",
    "max_z = 4.5\n",
    "z = np.linspace(-max_z, max_z, 200)\n",
    "\n",
    "plt.figure(figsize=(11, 3.1))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot([-max_z, 0], [0, 0], \"r-\", linewidth=2, label=\"Heaviside\")\n",
    "plt.plot(z, relu(z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.plot([0, 0], [0, 1], \"r-\", linewidth=0.5)\n",
    "plt.plot([0, max_z], [1, 1], \"r-\", linewidth=2)\n",
    "plt.plot(z, sigmoid(z), \"g--\", linewidth=2, label=\"Sigmoid\")\n",
    "plt.plot(z, np.tanh(z), \"b-\", linewidth=1, label=\"Tanh\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Activation functions\")\n",
    "plt.axis([-max_z, max_z, -1.65, 2.4])\n",
    "plt.gca().set_yticks([-1, 0, 1, 2])\n",
    "plt.legend(loc=\"lower right\", fontsize=13)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(z, derivative(np.sign, z), \"r-\", linewidth=2, label=\"Heaviside\")\n",
    "plt.plot(0, 0, \"ro\", markersize=5)\n",
    "plt.plot(0, 0, \"rx\", markersize=10)\n",
    "plt.plot(z, derivative(sigmoid, z), \"g--\", linewidth=2, label=\"Sigmoid\")\n",
    "plt.plot(z, derivative(np.tanh, z), \"b-\", linewidth=1, label=\"Tanh\")\n",
    "plt.plot([-max_z, 0], [0, 0], \"m-.\", linewidth=2)\n",
    "plt.plot([0, max_z], [1, 1], \"m-.\", linewidth=2)\n",
    "plt.plot([0, 0], [0, 1], \"m-.\", linewidth=1.2)\n",
    "plt.plot(0, 1, \"mo\", markersize=5)\n",
    "plt.plot(0, 1, \"mx\", markersize=10)\n",
    "plt.grid(True)\n",
    "plt.title(\"Derivatives\")\n",
    "plt.axis([-max_z, max_z, -0.2, 1.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Hands-On Example with SKLearn's MLPClassifier:**\n",
    "\n",
    "Let's now see how to implement an MLP with sklearn's MLPClassifier.  Note that it is possible to use an MLP both as a regressor and as a classifier, but there are few considerations when using network as a classifier.\n",
    "\n",
    "When designing a neural network for a classification task, the architecture, including how you set up the output layer and choose the loss function, is crucial to the network's performance. Here's how these components are typically configured in the context of `sklearn`'s `MLPClassifier`:\n",
    "\n",
    "### Output Layer Configuration\n",
    "\n",
    "1. **Binary Classification:**\n",
    "   - For binary classification problems, where the goal is to distinguish between two classes, the output layer typically consists of a single neuron.\n",
    "   - The output neuron usually has a sigmoid activation function that squashes the input value into a range between 0 and 1, representing the probability of the input being in one of the two classes.\n",
    "   - A threshold is then applied to this probability to make a discrete prediction; for example, if the output is greater than 0.5, the input is assigned to one class, otherwise to the other class.\n",
    "\n",
    "2. **Multiclass Classification:**\n",
    "   - For multiclass classification problems, where there are more than two classes, the output layer has as many neurons as there are classes.\n",
    "   - The softmax function is often used in this case. It is a generalization of the sigmoid function that can handle multiple classes. It converts the output of the last hidden layer into probability distributions over predicted output classes.\n",
    "   - Each neuron in the output layer corresponds to a class, and the softmax function ensures that the sum of the probabilities of all output neurons equals 1.\n",
    "\n",
    "<div class=\"warning\"  style='padding:1em; background-color:#f5deae; margin:1em;>\n",
    "\n",
    "**The Softmax Function**\n",
    "\n",
    "For a given vector $ \\mathbf{z} $ of raw scores for each class, the softmax function $ \\sigma $ is defined as:\n",
    "\n",
    "$$ \\sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}} $$\n",
    "\n",
    "where:\n",
    "- $ i $ is the index of a particular element of the output vector $ \\mathbf{z} $,\n",
    "- $ K $ is the total number of classes,\n",
    "- $ e $ is the base of the natural logarithm,\n",
    "- $ e^{z_i} $ is the exponential of the score for class $ i $,\n",
    "- $ \\sum_{j=1}^{K} e^{z_j} $ is the sum of the exponentials of all the scores.\n",
    "\n",
    "The output of the softmax function for each element $ \\sigma(\\mathbf{z})_i $ is a probability that the input belongs to class $ i $, given the raw scores in $ \\mathbf{z} $. The sum of all the probabilities from the softmax function will equal 1.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='background-color:#E9D8FD; color: #69337A; border: solid #805AD5 4px; border-radius: 4px; padding:0.7em; width:90%'>\n",
    "\n",
    "\n",
    "**The Softmax Function**\n",
    "\n",
    "For a given vector $ \\mathbf{z} $ of raw scores for each class, the softmax function $ \\sigma $ is defined as:\n",
    "\n",
    "$$ \\sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}} $$\n",
    "\n",
    "where:\n",
    "- $ i $ is the index of a particular element of the output vector $ \\mathbf{z} $,\n",
    "- $ K $ is the total number of classes,\n",
    "- $ e $ is the base of the natural logarithm,\n",
    "- $ e^{z_i} $ is the exponential of the score for class $ i $,\n",
    "- $ \\sum_{j=1}^{K} e^{z_j} $ is the sum of the exponentials of all the scores.\n",
    "\n",
    "The output of the softmax function for each element $ \\sigma(\\mathbf{z})_i $ is a probability that the input belongs to class $ i $, given the raw scores in $ \\mathbf{z} $. The sum of all the probabilities from the softmax function will equal 1.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Implementing Softmax Function\n",
    "\n",
    "Using the above definition, can you implement your own version of the softmax function?  If you do it right, your output should look something like:\n",
    "\n",
    "```\n",
    "Probabilities: [0.65900114 0.24243297 0.09856589]\n",
    "Sum of probabilities: 1.0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # You'll need to use numpy.  Look at the docs if you need to figure out how to exponentiate things\n",
    "# Define the softmax function here\n",
    "\n",
    "def softmax(logits):\n",
    "    pass # replace this with the actual implementation\n",
    "    # Hint: Often, it's useful here to subtract the max for numerical stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to test\n",
    "\n",
    "# Example logits (raw scores) for 3 classes\n",
    "logits = np.array([2.0, 1.0, 0.1])\n",
    "\n",
    "# Apply softmax to convert logits to probabilities\n",
    "probabilities = softmax(logits)\n",
    "\n",
    "print(f\"Probabilities: {probabilities}\")\n",
    "print(f\"Sum of probabilities: {np.sum(probabilities)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Loss Function Selection\n",
    "\n",
    "1. **Binary Cross-Entropy:**\n",
    "   - For binary classification, the binary cross-entropy (also known as log loss) is typically used. It measures the performance of a classification model whose output is a probability value between 0 and 1.\n",
    "   - Binary cross-entropy loss is ideal for binary classification as it penalizes false classifications with a logarithmic penalty.\n",
    "\n",
    "2. **Categorical Cross-Entropy:**\n",
    "   - In multiclass classification problems, categorical cross-entropy (or softmax loss) is used. It measures the performance of a model where the prediction is a probability distribution across different outputs.\n",
    "   - This loss function is appropriate for multiclass classification problems where only one result is the correct classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='background-color:#E9D8FD; color: #69337A; border: solid #805AD5 4px; border-radius: 4px; padding:0.7em; width:90%'>\n",
    "\n",
    "\n",
    "**Categorical Cross Entropy Loss Function**\n",
    "\n",
    "The categorical cross-entropy loss function is commonly used in multi-class classification problems. It measures the performance of a classification model whose output is a probability value between 0 and 1. The loss increases as the predicted probability diverges from the actual label. For a single data point, the categorical cross-entropy loss is given by:\n",
    "\n",
    "$$ L_i = - \\sum_{j=1}^{K} y_{ij} \\log(p_{ij}) $$\n",
    "\n",
    "where:\n",
    "- $ L_i $ is the loss for the $ i $-th observation.\n",
    "- $ K $ is the number of classes.\n",
    "- $ y_{ij} $ is a binary indicator of whether class $ j $ is the correct classification for observation $ i $.\n",
    "- $ p_{ij} $ is the predicted probability that observation $ i $ is of class $ j $.\n",
    "\n",
    "The total loss over a dataset of $ N $ observations is the average of $ L_i $ for all observations:\n",
    "\n",
    "$$ L = \\frac{1}{N} \\sum_{i=1}^{N} L_i $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise: Implementing Categorical Cross-Entropy\n",
    "\n",
    "Using the above, try to implement the categorical cross-entropy function.  If you're right, you should get output that looks like:\n",
    "\n",
    "```\n",
    "Losses: [0.35667494 0.22314355 0.51082562]\n",
    "Average Loss: 0.3635480359261127\n",
    "```\n",
    "\n",
    "Implement your function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def categorical_crossentropy(y_true, y_pred):\n",
    "    # Note that 'y_true' is a one-hot encoded array of the true class\n",
    "    # y_pred is a list of probabilities\n",
    "    # Clip predictions to avoid log(0) error. Clip both sides to not favour any class.\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    # Compute the cross-entropy\n",
    "    pass # Replace this line!\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following to test your code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# Example true labels and predictions for a 3-class classification\n",
    "y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])  # One-hot encoded true labels\n",
    "y_pred = np.array([[0.7, 0.2, 0.1], [0.1, 0.8, 0.1], [0.2, 0.2, 0.6]])  # Predicted probabilities\n",
    "\n",
    "# Calculate loss for each observation\n",
    "losses = np.array([categorical_crossentropy(y_true[i], y_pred[i]) for i in range(len(y_true))])\n",
    "\n",
    "# Calculate average loss\n",
    "average_loss = np.mean(losses)\n",
    "\n",
    "print(f\"Losses: {losses}\")\n",
    "print(f\"Average Loss: {average_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note***\n",
    "\n",
    "- In the preceding, we used `np.clip` to prevent taking the logarithm of 0, which would result in `-inf`. This also prevents taking the logarithm of a number very close to 1, which could lead to numerical instability. The clipping limits are set very close to 0 and 1 (but not exactly 0 or 1) to prevent these issues.\n",
    "\n",
    "This average loss value is the one that would be minimized during the training of a multi-class classification model using an optimization algorithm like gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Practical Implementation with MLPClassifier\n",
    "\n",
    "In `sklearn`'s `MLPClassifier`, you don't have to manually specify the activation function for the output layer or the loss function; this is handled automatically based on the labels provided to the classifier during training:\n",
    "\n",
    "- If you fit the `MLPClassifier` with a binary label (e.g., `[0, 1, 1, 0]`), it will internally configure its output layer with one neuron and use binary cross-entropy as the loss function.\n",
    "- If you provide labels for a multiclass problem (e.g., `[0, 2, 1, 2]` for three classes), it will set up the output layer with one neuron per class and use categorical cross-entropy.\n",
    "\n",
    "When you initialize the `MLPClassifier`, you have the option to specify parameters like `hidden_layer_sizes`, which determines the size and the number of hidden layers, and `activation`, which allows you to choose the activation function for the hidden layers (e.g., `relu`, `tanh`).\n",
    "\n",
    "Here's a brief code snippet showing how you might initialize an `MLPClassifier` for a binary classification problem:\n",
    "\n",
    "```python\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# For a binary classification problem\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=200)\n",
    "```\n",
    "\n",
    "And for a multiclass problem:\n",
    "\n",
    "```python\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# For a multiclass classification problem\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', max_iter=200)\n",
    "```\n",
    "\n",
    "In both cases, `sklearn` automatically takes care of setting up the output layer and loss function appropriately based on the data you provide when calling the `fit` method. The `solver` parameter specifies the optimization algorithm to use for weight optimization, where 'adam' is usually a good default choice, but you might also try 'sgd'.  We'll talk more about the different solvers later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_classes=4, random_state=1)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)\n",
    "\n",
    "# Initialize the MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, alpha=1e-4,\n",
    "                    solver='sgd', activation=\"logistic\", verbose=10, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "\n",
    "# Train the MLP\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
