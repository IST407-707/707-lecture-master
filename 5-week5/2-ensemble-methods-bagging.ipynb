{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n",
    "\n",
    "Simple models - logistic regression, decision trees, naive bayes - are often easy to interpret, but offer lack power and suffer from either high bias or high variance. One way to address this is gathering a large number of simple models together and aggregate the answers of these models to achieve a better, aggregate answer. In human systems, this has been referred to as the \"wisdom of the crowd,\" and it is one of the underpinnings of Western socio-political organizations, being the fundamental motivation for both democracy and the market.\n",
    "\n",
    "Ensemble methods, at their core, apply the wisdom of the crowds idea to ML.The intuition is simple: if different models, with their unique strengths and weaknesses, make different errors, then combining them can often average out their errors and yield a better prediction.\n",
    "\n",
    "#### **Wisdom of the Crowd: A Coin Toss Experiment**\n",
    "\n",
    "Consider a simple coin toss. If a slightly biased coin has a 51% chance of coming up heads and we toss it a few times, the results could be fairly erratic. It might come up heads 7 times out of 10 or just 3 times out of 10. However, as we toss the coin more and more, the ratio of heads will tend to get closer and closer to the 51%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "heads_proba = 0.51\n",
    "np.random.seed(42)\n",
    "coin_tosses = (np.random.rand(10000, 10) < heads_proba).astype(np.int32)\n",
    "cumulative_heads = coin_tosses.cumsum(axis=0)\n",
    "cumulative_heads_ratio = cumulative_heads / np.arange(1, 10001).reshape(-1, 1)\n",
    "\n",
    "plt.figure(figsize=(8, 3.5))\n",
    "plt.plot(cumulative_heads_ratio)\n",
    "plt.plot([0, 10000], [0.51, 0.51], \"k--\", linewidth=2, label=\"51%\")\n",
    "plt.plot([0, 10000], [0.5, 0.5], \"k-\", label=\"50%\")\n",
    "plt.xlabel(\"Number of coin tosses\")\n",
    "plt.ylabel(\"Heads ratio\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.axis([0, 10000, 0.42, 0.58])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "As seen in the plot, with a small number of tosses, the ratio can be far from 51%. But as the number of tosses increases, the ratio of heads tends to 51%. This is known as the **Law of Large Numbers**. \n",
    "\n",
    "The coin toss experiment gives us a hint towards a fundamental concept in ensemble learning: even if each individual prediction (or coin toss) is weak and unreliable, aggregating predictions can lead to a more robust and accurate result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Basic Ensemble Techniques**\n",
    "\n",
    "#### **Voting Classifiers**\n",
    "\n",
    "One of the simplest ensemble methods is the Voting Classifier. The idea behind the Voting Classifier is to combine conceptually different machine learning classifiers and use either the majority (hard vote) or the average predicted probabilities (soft vote) to predict the class labels.\n",
    "\n",
    "- **Hard Voting**: As the name suggests, each individual classifier in the ensemble \"votes\" for a class, and the class that gets the majority of votes is the prediction of the ensemble.\n",
    "\n",
    "- **Soft Voting**: Here, every individual classifier provides a probability estimate for each class. The class with the highest average probability across the classifiers is chosen as the prediction.\n",
    "\n",
    "Soft voting often achieves better results because it gives more weight to highly confident votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generating a dataset\n",
    "X, y = make_moons(n_samples=1000, noise=0.3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Defining the classifiers\n",
    "log_clf = LogisticRegression(random_state=42)\n",
    "svm_clf = SVC(probability=True, random_state=42) # 'probability=True' to enable soft voting\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "voting_hard_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('svm', svm_clf), ('tree', tree_clf)],\n",
    "    voting='hard')\n",
    "voting_hard_clf.fit(X_train, y_train)\n",
    "\n",
    "voting_soft_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('svm', svm_clf), ('tree', tree_clf)],\n",
    "    voting='soft')\n",
    "voting_soft_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluating classifiers\n",
    "for clf in (log_clf, svm_clf, tree_clf, voting_hard_clf, voting_soft_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Using the above code, try adding the other classifiers you know about: SGDClassifier, KNN, and NaiveBayes. Do your results improve with more classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Bagging (Bootstrap Aggregating)** \n",
    "\n",
    "#### **2.1 Principle of Operation** \n",
    "\n",
    "Bagging, which stands for **Bootstrap Aggregating**, is an ensemble method that aims to reduce the variance of an estimator by leveraging multiple instances of it. The central idea revolves around the following:\n",
    "\n",
    "- **Training instances** are randomly sampled **with replacement** from the dataset. This means that the same training instance can be sampled multiple times for one predictor and not at all for another. This method of sampling is termed \"bootstrapping.\"\n",
    "  \n",
    "- Every predictor in the ensemble is trained on a different bootstrap sample. This means that different instances of the predictor might be seeing slightly different data during their training.\n",
    "\n",
    "- Once the predictors are trained, the ensemble can make predictions for a new instance by simply aggregating the predictions of all predictors.\n",
    "  - The aggregation function is typically the **statistical mode** (i.e., the most frequent prediction) for classification or the **average** for regression.\n",
    "\n",
    "#### **2.2 Key Features**\n",
    "\n",
    "- **Inherent parallelism** An essential feature of bagging is that the individual predictors can be trained in parallel, as they are independent of each other, which makes bagging scalable and suitable for distributed computing.\n",
    "- **Reduction in Variance**: As individual predictors might overfit to their specific bootstrap samples, when predictions from all the predictors are aggregated, the ensemble can generalize better and produce a more stable and accurate result.\n",
    "\n",
    "- **Maintains Bias**: The bias remains similar to that of a single predictor because each one is trained on data sampled from the entire dataset.\n",
    "\n",
    "#### **2.3 Bagging with `sklearn`**\n",
    "\n",
    "`sklearn` provides the `BaggingClassifier`, which automates the process of training multiple instances of a predictor on different subsets of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n",
    "                            max_samples=100, n_jobs=-1, random_state=42)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n",
    "def plot_decision_boundary(clf, X, y, alpha=1.0):\n",
    "    axes=[-1.5, 2.4, -1, 1.5]\n",
    "    x1, x2 = np.meshgrid(np.linspace(axes[0], axes[1], 100),\n",
    "                         np.linspace(axes[2], axes[3], 100))\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    \n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3 * alpha, cmap='Wistia')\n",
    "    plt.contour(x1, x2, y_pred, cmap=\"Greys\", alpha=0.8 * alpha)\n",
    "    colors = [\"#78785c\", \"#c47b27\"]\n",
    "    markers = (\"o\", \"^\")\n",
    "    for idx in (0, 1):\n",
    "        plt.plot(X[:, 0][y == idx], X[:, 1][y == idx],\n",
    "                 color=colors[idx], marker=markers[idx], linestyle=\"none\")\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(r\"$x_1$\")\n",
    "    plt.ylabel(r\"$x_2$\", rotation=0)\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_decision_boundary(tree_clf, X_train, y_train)\n",
    "plt.title(\"Decision Tree\")\n",
    "plt.sca(axes[1])\n",
    "plot_decision_boundary(bag_clf, X_train, y_train)\n",
    "plt.title(\"Decision Trees with Bagging\")\n",
    "plt.ylabel(\"\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "When does it make sense to use a Bagging Classifier?  In the following, explore different data parameters to develop your intuition for which classifier makes sense in which situation. \n",
    "\n",
    "1. Gradually increase the noise in the data (using the noise parameter).  How do the different classifiers perform.  Why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X, y = make_moons(n_samples=300, noise=.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "y_train_pred_tree = tree.predict(X_train)\n",
    "print(\"Decision Tree Accuracy (train):\", accuracy_score(y_train, y_train_pred_tree))\n",
    "print(\"Decision Tree Accuracy (test):\", accuracy_score(y_test, y_pred_tree))\n",
    "\n",
    "\n",
    "\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_log = log_reg.predict(X_test)\n",
    "y_train_pred_log = log_reg.predict(X_train)\n",
    "print(\"Logistic Regression Accuracy (train):\", accuracy_score(y_train, y_train_pred_log))\n",
    "print(\"Logistic Regression Accuracy (test):\", accuracy_score(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Now, do the same thing in the following.  What do you notice. How do you explain your observations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "max_samples = 100\n",
    "\n",
    "X, y = make_moons(n_samples=300, noise=.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "bag_tree = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, random_state=42,max_samples=max_samples)\n",
    "bag_tree.fit(X_train, y_train)\n",
    "y_pred_bag_train = bag_tree.predict(X_train)\n",
    "y_pred_bag = bag_tree.predict(X_test)\n",
    "print(\"Bagging Decision Tree Accuracy (train):\", accuracy_score(y_train, y_pred_bag_train))\n",
    "print(\"Bagging Decision Tree Accuracy (test):\", accuracy_score(y_test, y_pred_bag))\n",
    "\n",
    "\n",
    "bag_log = BaggingClassifier(LogisticRegression(), n_estimators=500, random_state=42,max_samples=max_samples)\n",
    "bag_log.fit(X_train, y_train)\n",
    "y_pred_bag_log_train = bag_log.predict(X_train)\n",
    "y_pred_bag_log = bag_log.predict(X_test)\n",
    "print(\"Bagging Logistic Regression Accuracy (train):\", accuracy_score(y_train, y_pred_bag_log_train))\n",
    "print(\"Bagging Logistic Regression Accuracy (test):\", accuracy_score(y_test, y_pred_bag_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now go back and start increasing the `max_samples` parameter.  How do things change? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.4 Out-of-Bag (OOB) Evaluation:**\n",
    "\n",
    "When using bagging (like in a Random Forest), some instances may be sampled multiple times for one predictor, while others may not be sampled at all. The ones not sampled for a particular predictor are called \"out-of-bag\" instances for that predictor.\n",
    "\n",
    "- **How is it useful?** Since a predictor never sees the OOB instances during training, they can be used to evaluate the predictor's performance without the need for a separate validation set or cross-validation. In essence, each predictor in the ensemble is evaluated using different OOB instances.\n",
    "\n",
    "- **OOB Score**: `sklearn` has a mechanism where, if you set `oob_score=True` for a bagging classifier, after training, the classifier will automatically compute an OOB score which gives an estimate of the prediction accuracy using the OOB instances.\n",
    "\n",
    "- **Comparison with Cross-Validation**: While OOB can provide a good performance estimate, cross-validation is generally more robust as it averages performance over multiple train-test splits, giving a more comprehensive view of model stability and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=500, noise=.3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n",
    "                            oob_score=True, n_jobs=-1, random_state=42)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.4 Random Forests**\n",
    "\n",
    "Random Forests are a well-known powerful ML techniques that utilizes Decision Trees with bagging. However, in addition to using different bootstrap samples,  introduces randomness in feature selection when splitting a node. That is, instead of considering all features for a split, a random subset of features is chosen. There are several reasons for this:\n",
    "\n",
    "1. **Decorrelation of Trees**: One of the primary reasons for introducing randomness in feature selection at each node is to ensure that individual decision trees in the ensemble are not highly correlated. If we always chose the best features for splitting, many of the trees in the ensemble would look similar (especially at the top splits). This would diminish the benefits of ensemble averaging.\n",
    "\n",
    "2. **Reduced Overfitting**: By considering only a subset of features, we prevent the trees from always splitting on the most dominant features, which can lead to overfitting. Instead, the model is forced to consider other features, leading to more diverse and generalized decision boundaries.\n",
    "\n",
    "3. **Computational Efficiency**: Evaluating the best split is computationally costly. By considering only a subset of features, the training time for each tree is reduced. Given that Random Forests involve training many trees, this computational saving is significant.\n",
    "\n",
    "One significant advantage of Random Forest is that it can provide an estimate of the importance of each feature in making accurate predictions.  However, this comes at the cost of interpretability - because a random forest involves many trees, it's very hard to draw any insights (e.g. rules) about how predictions are being made.\n",
    "\n",
    "\n",
    "**Hyperparameters and Practical Considerations**\n",
    "\n",
    "\n",
    "- Number of Trees (`n_estimators`): More trees usually result in better performance but with diminishing returns. It also increases computational cost.\n",
    "- Max Depth (`max_depth`): Controls the depth of the trees. \n",
    "- Max Features (`max_features`): The number of features to consider for splitting. Influences decorrelation.\n",
    "- Minimum Samples Split (`min_samples_split`): Minimum samples required to make a new split.\n",
    "- Others: `min_samples_leaf`, `bootstrap`, etc.\n",
    "- The default settings in sklearn's RF implementation often provide a good starting point.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Play with the following example for a few moments to get a sense of how different parameter settings work.  See [the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) for additional detail on the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=500, max_depth=3, max_features='sqrt', oob_score=True, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Performance metrics\n",
    "print(rf_clf.oob_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note***: a RandomForestClassifier is identical to a BaggingClassifier with a DecisionTree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(max_features=\"sqrt\", max_leaf_nodes=16),\n",
    "    n_estimators=500, n_jobs=-1, random_state=42)\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=500, max_features='sqrt', max_leaf_nodes=16, random_state=42)\n",
    "\n",
    "\n",
    "# extra code â€“ verifies that the predictions are identical\n",
    "bag_clf.fit(X_train, y_train)\n",
    "rf_clf.fit(X_train,y_train)\n",
    "y_pred_bag = bag_clf.predict(X_test)\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "np.all(y_pred_bag == y_pred_rf)  # same predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1 Feature Importance with Random Forests\n",
    "\n",
    "Feature importance is a key benefit of Random Forests, as it provides insight into which features are the most influential in making predictions. Understanding feature importance can help in feature engineering, feature selection, and interpreting the model.  The Random Forest API makes it easy to inspect feature importance, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Get the indices of the features sorted by importance\n",
    "sorted_idx = np.argsort(importances)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.barh(range(X.shape[1]), importances[sorted_idx], align='center')\n",
    "plt.yticks(range(X.shape[1]), iris.feature_names)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importance using Random Forest')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a high-dimensional dataset, Random Forest's feature importance can be used to select a subset of the most influential features. This can speed up training and even improve model performance if there are noisy or redundant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "import pandas as pd\n",
    "\n",
    "# Create a selector object\n",
    "sfm = SelectFromModel(rf, threshold=0.25)  # only select features with an importance > 0.25\n",
    "sfm.fit(X, y)\n",
    "\n",
    "selected_mask = sfm.get_support()\n",
    "\n",
    "# Extracting the indices of the selected features\n",
    "selected_indices = np.where(selected_mask)[0]\n",
    "\n",
    "\n",
    "X_transformed = sfm.transform(X)\n",
    "pd.DataFrame(X_transformed,columns =np.array(iris.feature_names)[selected_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be particularly instructive when processing data with a visual interpretation.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 minutes to run!\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X_mnist, y_mnist = fetch_openml('mnist_784', return_X_y=True, as_frame=False)\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rnd_clf.fit(X_mnist, y_mnist)\n",
    "\n",
    "heatmap_image = rnd_clf.feature_importances_.reshape(28, 28)\n",
    "plt.imshow(heatmap_image, cmap=\"hot\")\n",
    "cbar = plt.colorbar(ticks=[rnd_clf.feature_importances_.min(),\n",
    "                           rnd_clf.feature_importances_.max()])\n",
    "cbar.ax.set_yticklabels(['Not important', 'Very important'], fontsize=14)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
