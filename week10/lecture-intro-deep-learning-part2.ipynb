{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing MLPs with Keras\n",
    "\n",
    "### Introduction to Keras and the Sequential API\n",
    "\n",
    "**What is Keras?**\n",
    "\n",
    "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. Initially developed as an independent project, Keras has now been integrated into TensorFlow as `tf.keras`, making it TensorFlow's official high-level API. It simplifies many aspects of creating and training neural networks, especially for beginners, due to its user-friendly and modular approach.\n",
    "\n",
    "- **High-Level API:** Keras abstracts away many of the complex details of working directly with TensorFlow, making it more accessible and easier to experiment with neural networks.\n",
    "- **Flexibility and Ease-of-Use:** Keras provides simple APIs for common machine learning tasks, making it easy to build and train models with just a few lines of code.\n",
    "- **Integration with TensorFlow:** As part of TensorFlow, Keras can leverage all the powerful features of TensorFlow, including distributed training, performance optimizations, and production-ready deployment.\n",
    "\n",
    "**Sequential API in Keras**\n",
    "\n",
    "One of the key features of Keras is its `Sequential` API, which allows for the easy stacking of layers to create neural networks:\n",
    "\n",
    "- **Sequential Model:** This is the simplest type of model in Keras, ideal for plain stacks of layers where each layer has exactly one input tensor and one output tensor.\n",
    "- **Ease of Creation:** Models can be constructed by simply adding layers in sequence, making it very intuitive, especially for standard feedforward neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='background-color:#E9D8FD; color: #69337A; border: solid #805AD5 4px; border-radius: 4px; padding:0.7em; width:90%'>\n",
    "\n",
    "\n",
    "**What is a tensor?**\n",
    "\n",
    "A tensor is a generalized mathematical concept that can be thought of as an extension of more familiar entities like scalars, vectors, and matrices to higher dimensions. In the context of deep learning and data representation, tensors are multi-dimensional arrays of numerical values and serve as the basic data structures.\n",
    "\n",
    "| A scaler, shape: [] | A vector, shape: [3] | A matrix, shape: [3,2] |\n",
    "| --------------------|----------------------|------------------------|\n",
    "|![scaler](assets/scalar.png) | ![vector](assets/vector.png) | ![matrix](assets/matrix.png)|\n",
    "\n",
    "### Different Types of Tensors:\n",
    "\n",
    "1. **Scalars (0D tensors):**\n",
    "   - A single number is considered a scalar (or 0-dimensional tensor). For example, `5` or `-3.2`.\n",
    "   - In Python, a scalar can be represented by a float or int type.\n",
    "\n",
    "2. **Vectors (1D tensors):**\n",
    "   - A vector is an array of numbers. It has one axis and is known as a 1D tensor. For example, `[1, 3, 5, 7]`.\n",
    "   - In Python, a 1D tensor can be represented using a list or a 1D array in NumPy.\n",
    "\n",
    "3. **Matrices (2D tensors):**\n",
    "   - A matrix has two axes (often referred to as rows and columns). It's a 2D tensor. For example, `[[1, 2, 3], [4, 5, 6]]`.\n",
    "   - In Python, a matrix is typically represented using a 2D NumPy array.\n",
    "\n",
    "4. **3D tensors and higher-dimensional tensors:**\n",
    "   - When you start stacking matrices in a new axis, you get a 3D tensor, which can be visualized as a cube of numbers.\n",
    "   - Continuing this pattern leads to higher-dimensional tensors (4D, 5D, etc.). In deep learning, these are common, especially in applications involving images, video, and other complex data types.\n",
    "\n",
    "| A 3-axis tensor, shape: [3, 2, 5]|||\n",
    "| --------------------|---|---|\n",
    "|![3-axis numpy](assets/3-axis_numpy.png) | ![3-axis front](assets/3-axis_front.png) | ![3-axis block](assets/3-axis_block.png)|\n",
    "\n",
    "\n",
    "### Tensors in Deep Learning:\n",
    "\n",
    "- **Data Representation:** In deep learning frameworks, tensors are used to represent the data. For instance, a 3D tensor could be used for time-series data, a 4D tensor for image data (including channels, such as color channels in an image), and a 5D tensor for video data (which has frames over time).\n",
    "\n",
    "- **Operations on Tensors:** Neural networks operations, like matrix multiplication, additions, and activation functions, are performed using tensors. These operations are efficiently implemented to handle high-dimensional data.\n",
    "\n",
    "- **Tensor Properties:**\n",
    "  - **Shape:** The dimensions of the tensor. For instance, a matrix with `n` rows and `m` columns has the shape `(n, m)`.\n",
    "  - **Data Type:** The type of data stored in the tensor (e.g., float32, int, etc.). In deep learning, it's often important to manage the data types to control memory usage and computational efficiency.\n",
    "\n",
    "- **Frameworks:** TensorFlow, as its name suggests, is designed around tensors. PyTorch also uses tensors as its fundamental data structure. These frameworks provide libraries to create, manipulate, and compute on tensors, usually with GPU acceleration for high efficiency.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Building a Simple MLP for the Fashion MNIST Dataset\n",
    "\n",
    "Let's go through the process of building a simple Multilayer Perceptron (MLP) using Keras to classify images from the Fashion MNIST dataset.\n",
    "\n",
    "1. **Dataset:** Fashion MNIST is a dataset of Zalando's article images, consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes (like T-shirts, trousers, bags, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set contains 60,000 grayscale images, each 28x28 pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each pixel intensity is represented as a byte (0 to 255):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's scale the pixel intensities down to the 0-1 range and convert them to floats, by dividing by 255:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test = X_train / 255., X_valid / 255., X_test / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can plot an image using Matplotlib's imshow() function, with a 'binary' color map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X_train[0], cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are the class IDs (represented as uint8), from 0 to 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the corresponding class names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first image in the training set is an ankle boot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a sample of the images in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 4\n",
    "n_cols = 10\n",
    "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
    "for row in range(n_rows):\n",
    "    for col in range(n_cols):\n",
    "        index = n_cols * row + col\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
    "        plt.axis('off')\n",
    "        plt.title(class_names[y_train[index]])\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Building the Model:**\n",
    "   - Import necessary modules from Keras.\n",
    "   - Initialize a Sequential model.\n",
    "   - Add layers (e.g., `Dense` layers for a fully connected network) with appropriate activation functions.\n",
    "   - Flatten the input data if necessary (since Fashion MNIST images are 2D)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways to create a model, differing only in syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# This is the simplest kind of Keras model for neural networks that are just composed of a single stack of layers connected sequentially. This is called the sequential API.\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Next, we specify the input shape, which doesn’t include the batch size, only the shape of the instances. Keras needs to know the shape of the inputs so it can determine the shape of the connection weight matrix of the first hidden layer.\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=[28, 28]))\n",
    "\n",
    "# The role of the Flatten layer is to convert each input image into a 1D array: for example, if it receives a batch of shape [32, 28, 28], it will reshape it to [32, 784]. In other words, if it receives input data X, it computes X.reshape(-1, 784). This layer doesn’t have any parameters; it’s just there to do some simple preprocessing.\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# Next we add a Dense hidden layer with 300 neurons. It will use the ReLU activation function. Each Dense layer manages its own weight matrix, containing all the connection weights between the neurons and their inputs. It also manages a vector of bias terms (one per neuron). When it receives some input data, it computes the activation at each neuron based on inputs from the previous layer.\n",
    "model.add(tf.keras.layers.Dense(300, activation=\"relu\")) \n",
    "\n",
    "# Etc. \n",
    "model.add(tf.keras.layers.Dense(100, activation=\"relu\")) # Second Dense layer with 100 neurons and ReLU \n",
    "\n",
    "#Here we need to specify as many neurons as we have classes, and use the \"softmax\" activation function because our classes are exclusive\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More concisely..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session() # extra code because we already built a model above – this clears the session to reset the name counters\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(300, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspecting the model**\n",
    "\n",
    "Keras provides numerous ways to inspect the model.  A good high level summary is available with `model.summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also look at the structure of the network graphically, but you'll need to have `graphviz` installed.  This is a horrendously long install process on Macs, so be prepared!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, \"my_fashion_mnist_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layers are stored in a list accessible via the `layers` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_layer('dense') is hidden1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even look at the raw weights and biases of the individual connections in any given layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = hidden1.get_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Compiling the Model:**\n",
    "   - Choose an optimizer (like 'adam' or 'sgd').\n",
    "   - Select an appropriate loss function (like 'sparse_categorical_crossentropy' for multi-class classification).\n",
    "   - Define metrics for monitoring (like 'accuracy')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.SGD(),\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4. **Training the Model:**\n",
    "   - Use the `fit` method on the model, passing the training data, number of epochs, and validation data (if available)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='background-color:#E9D8FD; color: #69337A; border: solid #805AD5 4px; border-radius: 4px; padding:0.7em; width:90%'>\n",
    "\n",
    "**Batches and Epochs**\n",
    "\n",
    "Batch size (`batch_size` in the `fit` method) and epochs are used to control the training process in a neural network, and so it's useful to have a good handle on these terms.\n",
    "\n",
    "### Batch Size\n",
    "\n",
    "When training a neural network, data is typically passed through the network in \"batches\". Each batch contains a subset of the total training data. The batch size is the number of samples present in one batch.\n",
    "\n",
    "- **Iterations:** An iteration is one forward pass and one backward pass of all the training examples in one batch. The number of iterations is equal to the number of batches needed to process the entire training set once.\n",
    "\n",
    "#### Why Does Batch Size Matter?\n",
    "\n",
    "1. **Memory Constraints:**\n",
    "   - Larger batches require more memory. With a limited amount of GPU or CPU memory, there's an upper bound to the batch size that can be used. If the batch size is too large, you may encounter memory errors.\n",
    "\n",
    "2. **Training Speed:**\n",
    "   - Larger batches often allow for more efficient computation. Modern deep learning libraries and hardware (like GPUs) are optimized for parallel computations that can process large batches of data more efficiently than smaller batches.\n",
    "\n",
    "3. **Convergence and Performance:**\n",
    "   - Batch size can affect the model's ability to converge to a global or local minimum and the quality of the minimum found.\n",
    "   - Smaller batches can offer a regularizing effect and less stable convergence, potentially leading to better generalization in some models.\n",
    "   - Larger batches provide a more accurate estimate of the gradient, but they can also lead to convergence to sharp minimizers that may not generalize as well.\n",
    "\n",
    "4. **Stochastic Gradient Descent and Variants:**\n",
    "   - With a batch size of 1, the algorithm is a pure Stochastic Gradient Descent (SGD), where each batch contains a single sample. This can lead to very noisy gradient updates and a longer time to converge, but potentially better generalization.\n",
    "   - Larger batch sizes approximate the gradient of the entire dataset more accurately, but the noise in the gradient estimate can sometimes help escape local minima.\n",
    "\n",
    "\n",
    "#### Choosing Batch Size\n",
    "\n",
    "The choice of batch size is often a balance between these factors:\n",
    "\n",
    "- **Memory Limitations:** Determined by your hardware.\n",
    "- **Convergence Properties:** Depending on the specific problem and model architecture, certain batch sizes may lead to better performance.\n",
    "- **Training Speed:** Larger batches may train faster per epoch but might require more epochs for convergence.\n",
    "\n",
    "In practice, batch sizes are often chosen based on empirical results and hardware constraints. Common batch sizes include 32 (the default in keras), 64, 128, 256, etc., but the optimal size can vary significantly depending on the specific application and dataset. Experimenting with different batch sizes is a key part of the model tuning process.\n",
    "\n",
    "### Epochs\n",
    "\n",
    "An \"epoch\" is a term used to describe one complete pass of the entire training dataset through the learning algorithm.\n",
    "\n",
    "   - During one epoch, every sample in the training dataset is presented once to the network, allowing the model to learn from the entire dataset.\n",
    "   - Each epoch consists of several iterations, depending on the batch size. For instance, if you have 1,000 training samples and a batch size of 100, it takes 10 iterations to complete one epoch.\n",
    "   - In each epoch, the weights of the network are updated multiple times through the backpropagation algorithm, gradually improving the model's performance on the training data.\n",
    "\n",
    "#### Why Do Epochs Matter?\n",
    "\n",
    "1. **Learning Process:**\n",
    "   - Multiple epochs are necessary because passing the entire dataset through the network once is usually not enough to learn all the features and patterns in the data. With each epoch, the model gets a chance to learn more about the data.\n",
    "\n",
    "2. **Convergence:**\n",
    "   - More epochs can lead to better convergence, meaning the model's loss on the training data (and hopefully on unseen data) decreases, and its accuracy improves.\n",
    "   - However, too many epochs can lead to overfitting, where the model starts to learn the noise in the training data, leading to poor generalization on new, unseen data.\n",
    "\n",
    "3. **Trade-offs:**\n",
    "   - **Computational Cost:** More epochs mean more computation, so training takes longer and consumes more resources.\n",
    "   - **Early Stopping:** This technique involves stopping the training process before the number of specified epochs is reached if the model stops showing improvement, which can save computational resources and prevent overfitting.\n",
    "\n",
    "4. **Hyperparameter Tuning:**\n",
    "   - The number of epochs is a hyperparameter that you may need to tune. The optimal number of epochs varies widely based on the specific problem, dataset, and neural network architecture.\n",
    "\n",
    "### Practical Implications\n",
    "\n",
    "- **Monitoring Performance:** During training, you should monitor the model's performance on a separate validation set to gauge how well it is learning. If performance on the validation set begins to degrade (i.e., validation loss increases or validation accuracy decreases), this may be a sign of overfitting.\n",
    "\n",
    "- **Flexibility:** In practice, the choice of the number of epochs is flexible and often determined by balancing time and computational resources with the need for sufficient model training.\n",
    "\n",
    "- **Learning Rate Scheduling:** Sometimes, adjusting the learning rate during training (e.g., reducing it gradually) can be more effective than simply setting a fixed number of epochs.\n",
    "\n",
    "In summary, epochs are a fundamental concept in the training of neural networks, balancing the amount of learning with computational efficiency and the risk of overfitting. The appropriate number of epochs is often found through experimentation and considering the specifics of each project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note here that instead of passing a validation set using the validation_data argument, you could set validation_split to the ratio of the training set that you want Keras to use for validation.  This can sometimes be more convenient\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that during each step, Keras will measure the loss and any metrics set at the end of each epoch.  If you pass an optional validation set, Keras will evaluate this as well.  This is really import to see how well your model is doing on held out data.  If performance on the training set is much better than the validation set, it's likely that you are overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5. **Evaluating the Model:**\n",
    "   - Evaluate the model's performance on the test data using the `evaluate` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note also that the `fit` method returns a history method that we can use to examine the training history. It contains the training parameters (history.params), the list of epochs it went through (history.epoch), and most importantly a dictionary (history.history) containing the loss and extra metrics it measured at the end of each epoch on the training set and on the validation set (if any). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(history.history).plot(\n",
    "    figsize=(8, 5), xlim=[0, 29], ylim=[0, 1], grid=True, xlabel=\"Epoch\",\n",
    "    style=[\"r--\", \"r--.\", \"b-\", \"b-*\"])\n",
    "plt.legend(loc=\"lower left\")  # extra code\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the model to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_proba.argmax(axis=-1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new = y_test[:3]\n",
    "y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7.2, 2.4))\n",
    "for index, image in enumerate(X_new):\n",
    "    plt.subplot(1, 3, index + 1)\n",
    "    plt.imshow(image, cmap=\"binary\", interpolation=\"nearest\")\n",
    "    plt.axis('off')\n",
    "    plt.title(class_names[y_test[index]])\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:  Hands-on with the Keras Sequential API\n",
    "\n",
    "Try building a MLP in Keras to fit the MNIST handwriting data. Here are some tips:\n",
    "\n",
    "- Make sure to flatten your images with a \"Flatten\" layer\n",
    "- Add one or two Dense layers with ReLU activation\n",
    "- Finish with a dense output layer with 10 neurons (one for each digit)\n",
    "- Compile the model with the `adam` optimizer, `sparse_categorical_crossentropy` loss, and `accuracy` metric\n",
    "- Use a validation_split parameter with 20 percent of your data for validation during training\n",
    "- Train with a reasonable number of epochs (e.g. 5-10) and evaluate your performance on the test set\n",
    "- Explore with different numbers of neurons, layers, and epochs!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just to get you started, here's some data!\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Tensorflows data is from 0-255, so here we just normalize to the 0-1 range:\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-keras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
